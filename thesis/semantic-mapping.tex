\chapter{Semantic Mapping}\label{chap:semantic-mapping}

% motivation
Humans associate concepts to areas that relate to their functionality. That
association is either useful by explaining where to find certain objects as well to
understand the types of activities to perform on certain areas. 
Those human-concepts are meant to be meaningful for humans and by incorporating conceptual
knowledge about them it is expected to improve the agent ability to interact and communicate
with humans and to perform human-like tasks.

% semantic mapping definition
In this context semantic mapping is seen as the process of building a representation of
the environment which associates spatial entities with a set of defined spatial concepts (in this case human concepts).

% Importance of spatial and conceptual knowledge
In order to build such a semantic map, it is important to define the spatial and conceptual knowledge
that allows an agent to map between its machine-friendly representation of the world and the high-level semantics
characteristic of the human world.
That knowledge needs to define and describe how to segment and identify concepts from the low-level representation.
It also needs to define how the concepts relate to each other and attribute meanings to them.
In the case of indoor environments this can be seen for example as: rooms are a unit of division of area and are
often separated by doors, each room can also be characterized by the objects inside, size and shape, and they
can be categorized in meaningful concepts such as: kitchen, library, corridor, etc\dots

% Why to treat it in a probabilistic way
The low-level representation and the knowledge of the agent, will always be to a certain a degree imperfect,
incomplete, inaccurate and invalid. Due to that both the concepts and their relations
cannot be quantified in a rule base system and ought to be treated in a probabilistic way in order to allow
the agent to still reason with some accuracy about the world.

% here a here a method is used that does semantic mapping and represents conceptual knowledge
% on a probabilistic fashion.
This thesis motivation lies on developing methods for detecting knowledge gaps on the spatial
semantic knowledge of a mobile agent. For that, spatial knowledge needs to be defined, as well
the methods used to perform the semantic mapping.
In that context this chapter presents \gls{Dora}, a system that represents its spatial knowledge
with the representation proposed by \cite{pronobis2010ias}, and performs semantic mapping using
inference on a probabilistic graphical model.


\section{Architecture Overview}
Pronobis~\cite{pronobis2011phd} presents a system architecture working in indoor environments using
non\hyp{}omnidirectional laser and visual sensors for semantic mapping. The system integrates cues such as geometry,
object presence and appearances and is able to perform inference across any semantic properties,
for example for the purpose of place categorization.
The introduced system is built around a spatial knowledge representation~\cite{pronobis2010ias} and it has not only been tested on real-scenarios
and performance tested across several conditions~\cite{pronobis2007iros} but also shown
to be tailored to effectively solve tasks arising on mobile robotics~\cite{hanheide2011ijcai}.

The \emph{spatial knowledge representation} has been drawn with an high-focus on probabilistic
and uncertain reasoning, human interaction and life-long learning capabilities.
For that it was considered an excellent base defining not only the concepts and
requirements of an high-level semantic representation but also the information flow
between all the layers involved on the agent.

\gls{Dora}, the system, utilizes its spatial knowledge to perform semantic mapping of the environment
using probabilist inferences on top of a graphical model.
Due to its nature, it was considered suitable for implementing novelty detection of semantic concepts.

\section{Spatial Knowledge Representation}
\cite{pronobis2010ias} proposes a spatial knowledge representation sub-divided into four layers.
Each layer focus on different aspects of the world, abstractions levels of the spatial knowledge
and different spatial scales. Each uses different spatial entities and relate to the agent
goals in different ways:

In the lowest abstraction level the sensory layer represents the most immediate and short term
accurate representation of the world. Above, the place layer discretizes the continuous
space into. The categorical layer focus on knowledge of low\hyp{}level, long\hyp{}term
categorical models of the sensory information. And at the higher level the conceptual map
associates concepts to areas such as rooms (see \autoref{fig:spatial-knowledge}).

\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{figures/spatial-knowledge-representation.pdf}
\caption{\label{fig:spatial-knowledge}Layered structure of the spatial representation.}
\end{figure}

\subsection{Sensory Layer}
The sensory layer keeps a detailed representation of the immediate surrounds of the agent.
That representation is based on directly sensed inputs as well on data-fusion for short
time\hyp{}intervals.

It is responsible for providing the agent with an exact position and to accurately position
low\hyp{}features and landmarks. The information is stored with associated uncertainty it
is highly susceptible to be replaced with newer versions. Also old or distant information
is forgotten.
By tracking the surrounds for short amount of times it provides an accurate representation
that can be used to locate and guide the agent on low\hyp{}level movements.

\subsection{Place Layer}
The place layer is responsible for maintaining a bottom\hyp{}up discretization of the
continuous space. On it the world is represented as a collection of discrete spatial
entities, called places, together with connectivity information.

Each place is defined with the features from the sensory layer and with spatial relations
to other places. Connectivity here, stands for ability to travel directly between the
places. Additionally places can be created for areas not yet explored allowing to introduce
virtual place holders to places that would eventually be uncover if the agent moved there.

The places are also considered stable on long\hyp{}term and can be used to help the agent
performing localization and planning longer distance motion planning. This higher level
representation although uncertain allows to model connectivity and relaxed spatial relations.

\subsection{Categorical Layer}
The categorical layer holds the generic long-term, low-level representations of categorical models
of the agent's sensory information.
The knowledge represented by this layer is not instance specific. It is the knowledge
required to determine certain features indicate the presence of an entity of certain
category. E.g.\ the knowledge required to detect objects, landmarks or describe room appearances.
Other properties may as well be defined for example shape, color, edges and size properties
are all defined in terms of low\hyp{}level features.

This layer is responsible for representing knowledge on how to extract properties used
by the conceptual layer.
Although not mandatory, in many cases the categories represented by it will map to human concepts
and not to the internal concepts of the agent.
The type of properties handled by this layer may require incredible complex models and for
that they might be trained on a supervised fashion.

\subsection{Conceptual Layer}
The conceptual layer provides an ontology that represents a taxonomy of the spatial concepts
and their relations with the properties represented on the categorical layer.
This associates semantic meaning to those properties that is useful for human-agent iteration
and provides relations that can be verbalized and explained with the ontology.

It provides also knowledge between the semantic concepts and their instances of those concepts.
Including definitions of spatial concepts related to space segmentation as well to semantic
categorization of those spatial entities.
E.g.\ rooms are commonly split by doors, rooms exist in a floor, a building has floors, milk is
likely to be found on kitchens, etc\dots

By providing information for segmentation, semantic mapping to concepts and relations between those
concepts, the knowledge represented on this layer allows the agent to explain and reason close to
the human concepts. The knowledge represented here is considered to be valid through very long
periods of time or even life\hyp{}long.


\section{System Structure}
The \Gls{Dora} system~\cite{hanheide2011ijcai} consists of several co-operating
sub-systems~(see \autoref{fig:dora-architecture}),
all of which actively use or maintain the \emph{spatial knowledge representation}.

The layer structure of the spatial knowledge representation allow to implement
data driven processes that control the flow of knowledge and information between the
lower and higher\hyp{}levels layers. To reduce computation and make it feasible not
all updates from lower\hyp{}levels are treated immediately. Instead the system waits
until a substantial change has occurred.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{figures/dora-architecture.pdf}
\caption{\label{fig:dora-architecture}Dora the Explorer as well as the elements and the data flow inside
         the semantic mapping system.}
\end{figure}

Initially the low-level topology and metric mapping systems use a \gls{SLAM} algorithm~\cite{Folkesson07a}
that is used as representation on the sensory layer. Those two systems are also responsible
for creating the place map on the place layer, as that is highly related with the \gls{SLAM} goals.
By using the landmarks (doors) and other available information the system also tries to
group places and understand the concept of rooms.

At the same time the spatial property categorization system is responsible for detecting
instances of the categorical knowledge by running classifiers previously trained to detect
a set of features listed on \autoref{sec:dora-features}. For that it uses the data available
from a laser range finder and a camera and besides classifying the features it estimates also
its confidence on the information.
The extracted features together with the estimates is then integrated over time and space
in order to increase its robustness.

Finally the conceptual map system utilizes the acquired instance knowledge together with
the high\hyp{}level conceptual knowledge to create a probabilistic graphical model that allows
to run inferences.
Since this conceptual mapping processing is responsible for creating the probabilistic
graphical model it is described in detail in \autoref{sec:conceptual-map}.


\subsection{Features}
\label{sec:dora-features}
The \emph{categorical layer} requires classifiers with the ability to categorize the
low\hyp{}level features into categories.
For that \gls{Dora} performs detection on the following categorical entities:
\begin{description}
\item[Doorway Detection] by using the laser range finder the system tries to detect doors.
\item[Geometric Shape] by using features extracted from the laser data and pre-trained classifiers
the system tries to classify rooms according to their shapes (e.g.\ rectangular, square).
\item[Geometric Size] is classified once again recurring to laser data. In this case the system
tries to categorize the room size according to a set of previously learned concepts (e.g. large, medium, small).
\item[Object Detection] is performed by running traditional object recognition classifiers.
Those are previously trained on a supervised fashion to later be able to detect objects
based on their visual features, e.g.\ by using \gls{SIFT} as described in \autoref{sec:sift}.
\item[Visual Appearance] is extracted from the visual input. For that Dora uses a trained
a set of classifiers on global visual features such as \gls{CRFH} to categorize the visual
appearance of a room in categories such as corridor, office, library.
\end{description}

\section{Conceptual Map}
\label{sec:conceptual-map}
As \gls{Dora} moves through the environment it builds a \emph{conceptual map}: a structural and
probabilistic representation of the space instantiated as a \emph{graphical model}.
It includes taxonomy of human\hyp{}compatible spatial concepts which are linked to the sensed 
instances of these concepts drawn from lower layers. It is the conceptual layer which 
contains the information that kitchens commonly contain cereal boxes and have certain 
general appearance and allows the robot to infer that the cornflakes box in front of the 
robot makes it more likely that the current room is a kitchen. The conceptual layer is 
described in terms of a probabilistic ontology defining spatial concepts and linking 
those concepts to instances of spatial entities (see the example of the ontology in
\autoref{fig:spatial-knowledge}).

Based on this design, a \emph{chain graph} model is proposed as a 
representation for performing inferences on the knowledge represented in the conceptual 
layer. Chain graphs are probabilistic graphical models that combine the properties of 
both Bayesian Networks and Random Markov Fields. This results in an efficient approach to 
probabilistic modeling and reasoning about conceptual knowledge.

\begin{figure}[h]
\centering
\includegraphics[width=0.75\textwidth]{figures/chaingraph.pdf}
\caption{\label{fig:chain-graph}Example chain-graph instantiated from the \emph{conceptual layer}.}
\end{figure}

%The conceptual layer structures the sensed environment together with the conceptual knowledge
%in order to create a structured probabilistic representation of the world.
An exemplary chain graph corresponding to the conceptual map ontology is presented
in \autoref{fig:chain-graph}. 
Each discrete place identified in the environment is represented by a set of random variables, 
one for each class of relation linked to that place. These are each connected to a random variable
over the categories of rooms, representing the ``is-a'' relation between rooms and their categories. 
Moreover, the room category variables are connected by undirected links to one another according 
to the topological map. The remaining variables represent: shape and appearance properties of space 
as observed from each place, and the presence of objects. 
These are connected to observations of features extracted directly from 
the sensory input. Finally, the 
distributions $p_{s}(\cdot|\cdot)$, $p_a(\cdot|\cdot)$, $p_{o_i}(\cdot|\cdot)$ 
represent the common sense knowledge about shape, appearance, and object co-occurrence, respectively. 
They allow for inference about other properties and room categories e.g. that the room is likely to be a kitchen,
because you are likely to have observed cornflakes in it. 

The use of graphical models to describe distributions of variables has useful properties.
First, they permit inference about uncertain conceptual knowledge. At the same time, they are 
generative models and therefore allow to calculate the probability
on any given subset of variables of the graph, allowing the system to work even when some
information is missing.

\subsection{Factor Graphs}
Although the conceptual layer works with \emph{chain graphs}~\cite{lauritzen2002chain},
those can be converted into \emph{factor graphs}~\cite{kschischang2001factor}.
Factor graphs have been introduced in \autoref{sec:graphical-models} and are used
throughout this thesis as they provide an easier manipulation due to factorization.

Moreover, there exist efficient implementations of inference engines operating on factor
graph representations~\cite{Mooij_libDAI_10}.
Describing the distribution function in terms of graphs permits the use of those engines to
efficently calculate marginals on any given subset of variables by exploiting conditional
independence between variables.

\subsection{Uncertain Sensing and Implicit Factors}
\label{sec:cues-from-low-level}
In a realistic scenario, classification tasks are, by their nature, uncertain.
And although a trained classifier is able to decide which class is more likely, there is
interest in handling uncertainty produced by those to make decisions more robust.

By modelling the classifiers as an implicit function $\phi_{classifier}(c, x)$ it becomes
possible to model the uncertainty of the decision given the presence of the sensed low-level
features $x$. The presence of those implicit models no longer allow calculation of the
normalization factor over the whole graph as there is no explicit model for either the
low\hyp{}level features $X$ (e.g.\ $X$ can be an image received from a visual sensor)
or for $\phi_{classifier}$ (e.g.\ the classifier can be an \gls{SVM}).

Nonetheless it is still possible to calculate any probability given that the features $X$
are observed. For that instead of modelling the variables $X$ and explicitly describing
the factor $\phi_{classifier}$ it is enough to replace it by an observed factor
$\phi_{classifier}(c|x)$ as shown on \autoref{fig:implicit-factor}.


\begin{figure}[h]
\centering
\subfloat[]{
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=20pt, row sep=10pt,ampersand replacement=\&] {
    \& \& \node (graph) [] {\dots}; \& \& \\
    \& \& \node (prop) [latent] {$c$}; \& \& \\
    \& \& \node (fac) [factor] {}; \& \& \\
    \& \& \& \& \\
    \node (x1) [obs] {$x_1$}; \&
    \node (x2) [obs] {$x_2$}; \&
    \node (x3) [obs] {$x_3$}; \&
    \node (xi) [] {\dots}; \&
    \node (xn) [obs] {$x_n$}; \\
  };
  \draw [-] (prop) -- (graph);
  \draw [-] (prop) -- (fac);
  \draw [-] (fac) -- (x1);
  \draw [-] (fac) -- (x2);
  \draw [-] (fac) -- (x3);
  \draw [-] (fac) -- (xi);
  \draw [-] (fac) -- (xn);

  \node (captfactor) [right=2pt of fac] {\footnotesize{$\phi_{classifier}(c, x)$}};
\end{tikzpicture}
}
\qquad
\subfloat[]{
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=20pt, row sep=10pt,ampersand replacement=\&] {
    \& \& \node (graph) [] {\dots}; \& \& \\
    \& \& \node (prop) [latent] {$c$}; \& \& \\
    \& \& \node (fac) [factor] {}; \& \& \\
    \& \& \node (hidden) [latent,draw=none] {}; \& \& \\
  };
  \draw [-] (prop) -- (graph);
  \draw [-] (prop) -- (fac);
  \node (captfactor) [right=2pt of fac] {\footnotesize{$\phi_{classifier}(c | x)$}};
\end{tikzpicture}

}

\caption{\label{fig:implicit-factor}By modelling classifiers as implicit factors it is
possible to perform inference on the graphical models handling the uncertainty measures
produced by those classifiers.}
\end{figure}

