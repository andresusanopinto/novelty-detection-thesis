\chapter{Novelty Detection on Semantic Representations}\label{chap:novelty}
The semantic mapping representation presented in \autoref{chap:semantic-mapping}
permits the agent to understand and reason over the human semantic concepts of
space. By using semantic data not only it eases the process of communicating 
but also reduces complexity and allows easy implementation of high-level
decisions.
Due to the nature of semantic data, variables have to be considered
uncertain. For that reason the whole semantic representation is instantiated
as a probabilistic structure: a \emph{conceptual map}. 

Although the conceptual map permits uncertain reasoning it does not incorporates
any detection mechanism for identifying or copping with knowledge gaps.
This means the system needs to consider the knowledge absolute and is not
able to detect by itself that some of it does not correctly describes reality. 

The main goal of this thesis is to address this issue by developing methods
able to detect both novel classes and novel structures on the semantic
representation. Allowing a more efficient behaviour when performing on unknown
and new environments. It is an important milestone on developing of high-level
knowledge and active learning.


\section{Detecting Novelty on a Single Variable}
This section focus on detecting a novel class on single variable of the
conceptual map. As seen before on \autoref{chap:novelty-intro},
assuming that $P(\overline{novel})$ is constant, novelty detection can be
done by implementing a threshold on $P(x|\overline{novel})/P(x)$.
In order to correctly measure novelty on a single variable both
conditional and unconditional probabilities need to be coherently defined:

The conceptual layer builds a \emph{probabilistic graphical model} $G$ that
represents the distribution of the sensed features $x$ assuming that both the
known structure and the known classes of all variables hold in the sample.
The correct approach is then to use $P_G(x)$ as an approximation for
$P(x|\overline{novel})$.

\begin{figure}[h]
\centering
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=15pt, row sep=10pt,ampersand replacement=\&] {
    \& \& \node (room3) [latent] {$c$}; \& \\
    \& \& \& \\
    \&
    \node (room1) [latent] {$a$}; \& \&
    \node (room2) [latent] {$b$}; \\
    \& \& \& \\
    \node (shape1f) [factor] {}; \&
    \node (appearance1f) [factor] {}; \&
    \node (object1f) [factor] {}; \&
    \node (factor2) [factor] {}; \\
    \node (shape1l) [obs] {$S_p$}; \&
    \node (appearance1l) [obs] {$A_p$}; \&
    \node (object1l) [obs] {$O_p$}; \&
    \node (prop2) [obs] {$X_p$}; \\
  };

  \draw [-] (room1) -- (shape1f) -- (shape1l);
  \draw [-] (room1) -- (appearance1f) -- (appearance1l);
  \draw [-] (room1) -- (object1f) -- (object1l);
  \draw [-] (room2) -- (factor2) -- (prop2);

  \draw [-] (room1) -- (room2) node (r12f) [midway,factor] {};
  \draw [-] (room1) -- (room3) node (r13f) [midway,factor] {};
  \draw [-] (room2) -- (room3) node (r23f) [midway,factor] {};

  \begin{pgfonlayer}{background}
    \plate{places1}{(shape1f)(shape1l)(object1l)}{$\forall p \in places(room1)$}{};
    \plate{places2}{(factor2)(prop2)}{$\dots$}{};
  \end{pgfonlayer}
\end{tikzpicture}

\caption{Factor graph modelling the probability distribution in the case both
          the variables types and graph structure hold true.}
\end{figure}


Assume now that everything the conceptual layer knows holds true except the
class of a single variable $a$, which the system has no information on. 
In that case, the correct approach is to replace the factors associated with
$a$ with a single factor connecting all the variables dependent on it as seen
on Figure~\ref{fig:novel-a-correct}.

Such an highly connected factor is hard to model. Though assuming the structure
holds true, it is possible to approximate that highly connected factor with a
variable $a^*$ with $K$ states.\footnote{Ideally those $K$ states would be all
the classes of $a$.}
Assuming no information is known on the unconditional distribution, the factors
dependent on $a^*$ should be modelled with uniform factors and $K$ can be set to
$1$.

Let $U_a$ be this new graph based on $G$ with variable $a$ replaced by $a^*$.
$P_{U_a}(x)$ models then the unconditional distribution of the feature set $x$
without the assumption that $a$ is one of the known classes and can be is used
as an approximation for $P(x)$.

\begin{figure}[h]
\centering
\subfloat[\label{fig:novel-a-correct}Graphical model able to correctly represent
          the unconditional probability when variable $a$ can be novel.]{
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=15pt, row sep=10pt,ampersand replacement=\&] {
    \& \& \node (room3) [latent] {$c$}; \& \\
    \& \& \& \\
    \&
    \node (room1) [factor] {}; \& \&
    \node (room2) [latent] {$b$}; \\
    \& \& \& \\
    \&
    \&
    \&
    \node (factor2) [factor] {}; \\
    \node (shape1l) [obs] {$S_p$}; \&
    \node (appearance1l) [obs] {$A_p$}; \&
    \node (object1l) [obs] {$O_p$}; \&
    \node (prop2) [obs] {$X_p$}; \\
  };

  \draw [-] (room1) -- (shape1l);
  \draw [-] (room1) -- (appearance1l);
  \draw [-] (room1) -- (object1l);
  \draw [-] (room2) -- (prop2);

  \draw [-] (room1) -- (room2);
  \draw [-] (room1) -- (room3);
  \draw [-] (room2) -- (room3) node (r23f) [midway,factor] {};

  \begin{pgfonlayer}{background}
    \plate{places1}{(shape1f)(shape1l)(object1l)}{$\forall p \in places(room1)$}{};
    \plate{places2}{(factor2)(prop2)}{$\dots$}{};
  \end{pgfonlayer}
\end{tikzpicture}
}
\qquad
\subfloat[By using independent features for modelling the unconditional
          distribution an highly simplified graph is obtained.]{
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=15pt, row sep=10pt,ampersand replacement=\&] {
    \&
    \&
    \node (room3) [latent] {$c$}; \& \\
    \& \& \& \\
    \&
    \node (room1) [latent,dashed] {$a^*$}; \& \&
    \node (room2) [latent] {$b$}; \\
    \& \& \& \\
    \nofactor {shape1f}{}{$U$} {right=0pt}; \&
    \nofactor {appearance1f}{} {$U$} {right=0pt}; \&
    \nofactor {object1f}{} {$U$} {right=0pt}; \&
    \node (factor2) [factor] {}; \\
    \node (shape1l) [obs] {$S_p$}; \&
    \node (appearance1l) [obs] {$A_p$}; \&
    \node (object1l) [obs] {$O_p$}; \&
    \node (prop2) [obs] {$X_p$}; \\
  };
  \draw [-] (shape1f) -- (shape1l);
  \draw [-] (appearance1f) -- (appearance1l);
  \draw [-] (object1f) -- (object1l);
  \draw [-] (room2) -- (factor2) -- (prop2);
  \draw [-,draw=none] (room1) -- (room2) node (r12f) [midway,factor] {};
  \draw [-,draw=none] (room1) -- (room3) node (r13f) [midway,factor] {};
  \draw [-] (room2) -- (room3) node (r23f) [midway,factor] {};
  \draw [-] (room2) -- (r12f);
  \draw [-] (room3) -- (r13f);
  \node (captr12f) [above=0pt of r12f] {\footnotesize{$U$}};
  \node (captr13f) [above=0pt of r13f] {\footnotesize{$U$}};

  \draw [-,dashed] (room1) -- (shape1f);
  \draw [-,dashed] (room1) -- (appearance1f);
  \draw [-,dashed] (room1) -- (object1f);
  \draw [-,dashed] (room1) -- (r12f);
  \draw [-,dashed] (room1) -- (r13f);

  \begin{pgfonlayer}{background}
    \plate{places1}{(shape1f)(shape1l)(object1l)}{$\forall p \in places(room1)$}{};
    \plate{places2}{(factor2)(prop2)}{$\dots$}{};
  \end{pgfonlayer}
\end{tikzpicture}
}
\caption{Factor graph modelling the probability distribution in the case
         where only variable $a$ is not necessarily a class the system has
         learned before.}
\end{figure}

Having now defined $P_G$ and $P_{U_a}$ as models models for
$P(x|\overline{novel})$ and $P(x)$, the ratio $P_G(x)/P_{U_a}(x)$ is used to
define an order-relation for the novelty of $a$.
All that is missing is a threshold $t_A$ to trigger on.
Such a threshold can be obtained by calculating which value of $t_A$
triggers a desired amount of the labelled data.
Also, since there is no reason to distinguish between different instances of
a variable type, only one threshold per variable type needs to be trained.

Concluding: assuming a constant $P(\overline{novel})$ for any specific set and
structure of $x$, novelty on a variable $a$ of type $A$ should be detected when:

\begin{equation}
\label{eq:threshold-with-ta}
\frac{P_G(x)}{P_{U_a}(x)} < t_A
\end{equation}

where $t_A$ is a threshold for variables of type $A$, $P_G(x)$ denotes the
probability of the sensed variables $x$ being generated by a distribution
where $a$ is known, and $P_{Ua}$ denotes the probability of $x$ being generated
by a distribution where $a$ is not necessarily a class the system has
learned before.


\subsection{Novelty Detection as a MAP Operation}
Having interest in integrating the presented method to detect novelty of a
variable $a$ on a single graphical model some more steps can be taken.

The method presented before obtains an unconditional model
from the conditional model by replacing a variable $a \in \{A_1,\dotsc, A_n\}$
and the associated factors $\phi_a$ with a variable
$a^* \in \{A^*_1,\dots,A^*_k\}$ and related unconditional factors $\phi_{a^*}$
and leaving all the remaining variables and factors equal.

With this it becomes possible to merge both graphs into a single one $G$ by
merging $a$ and $a^*$ into $b$ and factors $\phi_a$ and $\phi_{a^*}$ into
$\phi_b$ as follows:

\[
b \in \{A_1,\dotsc,A_n,A^*_1,\dotsc,A^*_k\}
\]

\[
\phi_b(b, \cdot) = \left\{
  \begin{array}{l l}
    \phi_a(b, \cdot), & \quad \text{if } b \in \{A_1,\dotsc,A_n\} \\
    \phi_{a^*}(b, \cdot), & \quad \text{if } b \in \{A^*_1,\dotsc,A^*_k\}
  \end{array} \right.
\]

Additionally it is possible to control variable $b$ states into being only one
of the states of $a$ or $a^*$ by introducing a control variable $c \in {0, 1}$
and factor $\phi_{c}$:
\[
\phi_{c}(b, c) = \left\{
  \begin{array}{l l}
    1, & \qquad \text{if } c = 0 \text{ and } b \in \{A_1,\dotsc,A_n\} \\
    1, & \qquad \text{if } c = 1 \text{ and } b \in \{A^*_1,\dotsc,A^*_n\}\\
    0, & \qquad \text{otherwise}
  \end{array} \right.
\]

By conditioning the control variable $c$ it is possible to obtain a graph with
potentials on any of the remaining variables equivalent to the potentials
obtained with $G_A$ (i.e.\ $c=0$) or $G_{U_a}$ (i.e.\ $c=1$). By introducing
an extra threshold factor $\phi_t(c)$ it also becomes possible to balance the
potential of the distribution described by $B$ such that a \gls{MAP} operation
on the $c$ variable decides on the novelty of $a$ with a desired threshold:

\begin{eqnarray*}
 P_B(c=0 | x)         &<& P_B(c=1 | x) \\
\Phi_B(c=0 \land x)   &<& \Phi_B(x=1 \land x) \\
\Phi_G(x) \phi_t(c=0) &<& \Phi_{U_a}(x) \phi_t(c=1) \\
P_G(x) Z_G \phi_t(c=0) &<& P_{U_a}(x) Z_{U_a} \phi_t(c=1) \\
\frac{P_G(x)}{P_{U_a}(x)} &<& \frac{\phi_t(c=1)}{\phi_t(c=0)}\frac{Z_{U_a}}{Z_G}
\end{eqnarray*}

Concluding, after merging graph $G$ and $U_a$ into graph $B$, the condition
described in \autoref{eq:threshold-with-ta} can be test by performing a
\gls{MAP} operation on variable $c$ of graph $B$ when the threshold factor
$\phi_t$ is proportional to:
\begin{equation}
\phi_t(c) \propto \left\{
  \begin{array}{l l}
    1, & \qquad \text{if } c = 0 \\
    t_A \frac{Z_G}{Z_{U_a}}, & \qquad \text{if } c = 1\\
  \end{array} \right.
\end{equation}

This equation is not very practical due to the need of calculation of
normalization factors $Z_G$ and $Z_{U_a}$. The presence of this factors
not only turns computation more expensive, but also requires that a full model
is known for all the factors disabling the ability to incorporate probabilistic
sensing output from complex detectors such as \glspl{SVM}. Ultimately, it also
removes capability to mix complex thresholds on several variables that would
allow to do probabilistic inference on the novelty of multiple variables at once.

 
\begin{figure}[h]
\centering
lalla
\caption{\label{fig:merging-graphs}A condition based on the ratio between 2
         factor graphs that share some structure can be performed by testing the
         MAP probability of a variable on a merged graph.}
\end{figure}

Analysing the reason for the normalization factors playing a role on the
threshold can be tracked down to the assumption that $P(novel)$ is
considered constant over any set or structure of $x$.

This requirement forces the threshold function to use the normalization factors
to compensate for a bigger likelihood of certain structure being naturally more
disposed to generate novel categories.

This relation between graph structure and $P(novel)$ is discussed in more detail
in the next section where also the novelty detection method is made to work
without normalization factors, allowing probabilistic sensing, and extended to
detect novelty on multiple variables.

\section{Detecting and Copping with Novelty on Multiple Variable}
On the previous section it was shown how to detect a novel class on a variable
assuming that the rest of the graph variables and structure is correctly modeled
by the agent knowledge.
In this section methods are presented that not only allow to extended this to
detection on multiple variables but also allow the system to incorporate
novelty information back in the graphical model in order to continue its
probabilistic and uncertain reasoning.

When trying to extend the method to deal with multiple novel variables it may
be tempting to follow a deterministic approach such as:
when the agent determines a variable $a$ to have a novel class, it marks it as
novel and proceeds from that moment by using a graphical model where the factors
associated with the $a$ replaced by unconditional factors.
But such an approach would not be able to optimally detect novel variables
as later decisions are dependent on former ones.
It would also not be able to adapt itself and reconsider previously decisions
when more data becomes available for the agent.

To handle those issues a fully probabilistic approach should be taken
instead of a deterministic.
That can be done by extending all the variables on the factor graph representing
the semantic map with an additional state \emph{novel} and extending all the
factors with information on the unconditional distribution.

Such a factor graph would be able to handle novel variables and incorporate
prior-information on the unconditional distributions since it is able to
represent any state of the $X$ variables present on the graph where any
subset of them is novel. But in order to add the ability to control the
novelty thresholds a normalization factor $\phi_N$ needs to be introduced.

\begin{figure}[h]
\centering
Yeah get a figure for here.
\caption{\label{fig:multiple-big-threshold}Factor graph extended to be able to
         represent novelty on any subset of variables. Note the added factor
         $\phi_N$ that servers as a normalization factor such that probability
         with any subset of $Y$ variables considered novel can be scaled with
         any value such that any thresholding decision mechanism can be
         implemented.}
\end{figure}

In order to give full control on the threshold on which any subset of
variables is considered novel the factor $\phi_N$ needs to be dependent on all
the $X$ variables and act as a weighting function that correctly scales the
likelihood of a given instance of the $X$ variables.
If the criterium for defining the threshold is the likelihood of all variables
$X$ being non-novel has the max likelihood if and only if
\autoref{eq:multiple-threshold} holds for all the non-empty subsets $Y$ of $X$.
The weight for $\phi_N$ when the set $Y$ is classified as novel
can be set for $t_Y$ and $\phi_N$ for the empty subset of novel variables is set
to $1$.

\begin{equation}
\label{eq:multiple-threshold}
\frac{P_G(x)}{P_{U_Y}(x)} > t_Y
\end{equation}

Still without further assumptions a threshold $t_Y$ would need to be defined for
all the $2^N-1$ non-empty subsets.
By taking a simple approach of considering the novelty on a variable independent
from the novelty on any other variable $t_Y$ can be factorized:

\begin{equation}
t_Y = \prod_{i \in Y}{t_i}
\end{equation}

This factorization yields then the interesting property to allow to split the
threshold factor $\phi_N$ into $N$ different $\phi_{N_i}$ associated with each
variable of the graph. A graph able to handle novelty detection on several
variables with the described factorization is illustrated on
\autoref{fig:multi-factorized-threshold}.

\begin{figure}[h]
\centering
Yeah get a figure for here.
\caption{\label{fig:multi-factorized-threshold}Factor graph extended to be able
         to represent novelty on any subset of variables. Note the added factor
         $\phi_N$ has been split into a $\phi_{N_i}$ still it is able to perform
         detection on multiple variables according to the factorization
         described in the text.}
\end{figure}


To sum up, the method described above allows to integrate novelty detection
into the graph, providing the agent with tools to describe its belief on the
novelty of variables on the system and to continue working with such a
representation. The mechanism is also fully probabilistic and at no time
the agent needs to take an irrevocable decision on the novelty of a variable,
allowing the agent beliefs to change over time as more data becomes available.


\subsection{Handling novelty signals from the low-level classifiers}
On \autoref{sec:clues-from-low-level} it was shown that information from the
low-level classifiers is integrated in the conceptual map in a probabilistic
way by introducing a factor $\phi_o$ on each observed variable that describes
the probabilistic distribution of that variable given by the classifier.

Since in order to detect novelty all the variables have been extended with a
new state indicating novelty. That same state can be used to integrate novelty
signals coming from the low-level classifiers.

TODO: maybe describe how those novelty signals could be generated from the
low-level classifiers\dots decision boundary distance, kernel pca, etc\dots


\section{Detecting New Structure}

