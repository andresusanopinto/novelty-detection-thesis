\chapter{Novelty Detection on Semantic Representations}\label{chap:novelty}
The semantic mapping representation presented in \autoref{chap:semantic-mapping}
permits the agent to understand and reason over the human semantic concepts of
space. By using semantic data not only it eases the process of communicating 
but also reduces complexity and allows easy implementation of high-level
decisions.
Due to the nature of semantic data, variables have to be considered
uncertain. For that reason the whole semantic representation is instantiated
as a probabilistic structure: a \emph{conceptual map}. 

Although the conceptual map permits uncertain reasoning it does not incorporates
any detection mechanism for identifying or copping with knowledge gaps.
This means the system needs to consider the knowledge absolute and is not
able to detect by itself that some of it does not correctly describes reality. 

The main goal of this thesis is to address this issue by developing methods
able to detect both novel classes and novel structures on the semantic
representation. Allowing a more efficient behaviour when performing on unknown
and new environments. It is an important milestone on developing of high-level
knowledge and active learning.


\section{Detecting Novelty with Ratio Between Two Graphs}
This section focus on detecting a novel class on single variable of the
conceptual map. As seen before on \autoref{chap:novelty-intro},
assuming that $P(\overline{novel})$ is constant, novelty detection can be
done by implementing a threshold on $P(x|\overline{novel})/P(x)$.
In order to correctly measure novelty on a single variable both
conditional and unconditional probabilities need to be coherently defined:

The conceptual layer builds a \emph{probabilistic graphical model} $G$ that
represents the distribution of the sensed features $x$ assuming that both the
known structure and the known classes of all variables hold in the sample.
The correct approach is then to use $P_G(x)$ as an approximation for
$P(x|\overline{novel})$.

\begin{figure}[h]
\centering
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=15pt, row sep=10pt,ampersand replacement=\&] {
    \& \& \node (room3) [latent] {}; \& \\
    \& \& \& \\
    \&
    \node (room1) [latent] {$a$}; \& \&
    \node (room2) [latent] {}; \\
    \& \& \& \\
    \node (shape1f) [factor] {}; \&
    \node (appearance1f) [factor] {}; \&
    \node (object1f) [factor] {}; \&
    \node (factor2) [factor] {}; \\
    \node (shape1l) [obs] {$S_p$}; \&
    \node (appearance1l) [obs] {$A_p$}; \&
    \node (object1l) [obs] {$O_p$}; \&
    \node (prop2) [obs] {$X_p$}; \\
  };

  \draw [-] (room1) -- (shape1f) -- (shape1l);
  \draw [-] (room1) -- (appearance1f) -- (appearance1l);
  \draw [-] (room1) -- (object1f) -- (object1l);
  \draw [-] (room2) -- (factor2) -- (prop2);

  \draw [-] (room1) -- (room2) node (r12f) [midway,factor] {};
  \draw [-] (room1) -- (room3) node (r13f) [midway,factor] {};
  \draw [-] (room2) -- (room3) node (r23f) [midway,factor] {};

  \begin{pgfonlayer}{background}
    \plate{places1}{(shape1f)(shape1l)(object1l)}{$\forall p \in places(room1)$}{};
    \plate{places2}{(factor2)(prop2)}{$\dots$}{};
  \end{pgfonlayer}
\end{tikzpicture}

\caption{Factor graph modelling the probability distribution in the case both
          the variables types and graph structure hold true.}
\end{figure}


Assume now that everything the conceptual layer knows holds true except the
class of a single variable $a$, which the system has no information on. 
In that case, the correct approach is to replace the factors associated with
$a$ with a single factor connecting all the variables dependent on it as seen
on Figure~\ref{fig:novel-a-correct}.

Such an highly connected factor is hard to model. Though assuming the structure
holds true, it is possible to approximate that highly connected factor with a
variable $a^*$ with $K$ states.\footnote{Ideally those $K$ states would be all
the classes of $a$.}
Assuming no information is known on the unconditional distribution, the factors
dependent on $a^*$ should be modelled with uniform factors and $K$ can be set to
$1$.

Let $U_a$ be this new graph based on $G$ with variable $a$ replaced by $a^*$.
$P_{U_a}(x)$ models then the unconditional distribution of the feature set $x$
without the assumption that $a$ is one of the known classes and can be is used
as an approximation for $P(x)$.

\begin{figure}[h]
\centering
\subfloat[\label{fig:novel-a-correct}Graphical model able to correctly represent
          the unconditional probability when variable $a$ can be novel.]{
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=15pt, row sep=10pt,ampersand replacement=\&] {
    \& \& \node (room3) [latent] {}; \& \\
    \& \& \& \\
    \&
    \node (room1) [factor] {}; \& \&
    \node (room2) [latent] {}; \\
    \& \& \& \\
    \&
    \&
    \&
    \node (factor2) [factor] {}; \\
    \node (shape1l) [obs] {$S_p$}; \&
    \node (appearance1l) [obs] {$A_p$}; \&
    \node (object1l) [obs] {$O_p$}; \&
    \node (prop2) [obs] {$X_p$}; \\
  };

  \draw [-] (room1) -- (shape1l);
  \draw [-] (room1) -- (appearance1l);
  \draw [-] (room1) -- (object1l);
  \draw [-] (room2) -- (prop2);

  \draw [-] (room1) -- (room2);
  \draw [-] (room1) -- (room3);
  \draw [-] (room2) -- (room3) node (r23f) [midway,factor] {};

  \begin{pgfonlayer}{background}
    \plate{places1}{(shape1f)(shape1l)(object1l)}{$\forall p \in places(room1)$}{};
    \plate{places2}{(factor2)(prop2)}{$\dots$}{};
  \end{pgfonlayer}
\end{tikzpicture}
}
\qquad
\subfloat[By using independent features for modelling the unconditional
          distribution an highly simplified graph is obtained.]{
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=15pt, row sep=10pt,ampersand replacement=\&] {
    \&
    \&
    \node (room3) [latent] {}; \& \\
    \& \& \& \\
    \&
    \node (room1) [latent,dashed] {$a^*$}; \& \&
    \node (room2) [latent] {}; \\
    \& \& \& \\
    \nofactor {shape1f}{}{$U$} {right=0pt}; \&
    \nofactor {appearance1f}{} {$U$} {right=0pt}; \&
    \nofactor {object1f}{} {$U$} {right=0pt}; \&
    \node (factor2) [factor] {}; \\
    \node (shape1l) [obs] {$S_p$}; \&
    \node (appearance1l) [obs] {$A_p$}; \&
    \node (object1l) [obs] {$O_p$}; \&
    \node (prop2) [obs] {$X_p$}; \\
  };
  \draw [-] (shape1f) -- (shape1l);
  \draw [-] (appearance1f) -- (appearance1l);
  \draw [-] (object1f) -- (object1l);
  \draw [-] (room2) -- (factor2) -- (prop2);
  \draw [-,draw=none] (room1) -- (room2) node (r12f) [midway,factor] {};
  \draw [-,draw=none] (room1) -- (room3) node (r13f) [midway,factor] {};
  \draw [-] (room2) -- (room3) node (r23f) [midway,factor] {};
  \draw [-] (room2) -- (r12f);
  \draw [-] (room3) -- (r13f);
  \node (captr12f) [above=0pt of r12f] {\footnotesize{$U$}};
  \node (captr13f) [above=0pt of r13f] {\footnotesize{$U$}};

  \draw [-,dashed] (room1) -- (shape1f);
  \draw [-,dashed] (room1) -- (appearance1f);
  \draw [-,dashed] (room1) -- (object1f);
  \draw [-,dashed] (room1) -- (r12f);
  \draw [-,dashed] (room1) -- (r13f);

  \begin{pgfonlayer}{background}
    \plate{places1}{(shape1f)(shape1l)(object1l)}{$\forall p \in places(room1)$}{};
    \plate{places2}{(factor2)(prop2)}{$\dots$}{};
  \end{pgfonlayer}
\end{tikzpicture}
}
\caption{Factor graph modelling the probability distribution in the case
         where only variable $a$ is not necessarily a class the system has
         learned before.}
\end{figure}

Having now defined $P_G$ and $P_{U_a}$ as models models for
$P(x|\overline{novel})$ and $P(x)$, the ratio $P_G(x)/P_{U_a}(x)$ is used to
define an order-relation for the novelty of $a$.
All that is missing is a threshold $t_A$ to trigger on.
Such a threshold can be obtained by calculating which value of $t_A$
triggers a desired amount of the labelled data.
Also, since there is no reason to distinguish between different instances of
a variable type, only one threshold per variable type needs to be trained.

Concluding: assuming a constant $P(\overline{novel})$ for any specific set and
structure of $x$, novelty on a variable $a$ of type $A$ should be detected when:

\begin{equation}
\label{eq:threshold-with-ta}
\frac{P_G(x)}{P_{U_a}(x)} < t_A
\end{equation}

where $t_A$ is a threshold for variables of type $A$, $P_G(x)$ denotes the
probability of the sensed variables $x$ being generated by a distribution
where $a$ is known, and $P_{Ua}$ denotes the probability of $x$ being generated
by a distribution where $a$ is not necessarily a class the system has
learned before.


\subsection{Novelty Detection as a MAP Operation}
Having interest in integrating the presented method to detect novelty of a
variable $a$ on a single graphical model some more steps can be taken.

The method presented before obtains an unconditional model
from the conditional model by replacing a variable $a \in \{A_1,\dotsc, A_n\}$
and the associated factors $\phi_a$ with a variable
$a^* \in \{A^*_1,\dots,A^*_k\}$ and related unconditional factors $\phi_{a^*}$
and leaving all the remaining variables and factors equal.

With this it becomes possible to merge both graphs into a single one $G$ by
merging $a$ and $a^*$ into $b$ and factors $\phi_a$ and $\phi_{a^*}$ into
$\phi_b$ as follows:

\[
b \in \{A_1,\dotsc,A_n,A^*_1,\dotsc,A^*_k\}
\]

\[
\phi_b(b, \cdot) = \left\{
  \begin{array}{l l}
    \phi_a(b, \cdot), & \quad \text{if } b \in \{A_1,\dotsc,A_n\} \\
    \phi_{a^*}(b, \cdot), & \quad \text{if } b \in \{A^*_1,\dotsc,A^*_k\}
  \end{array} \right.
\]

Additionally it is possible to control variable $b$ states into being only one
of the states of $a$ or $a^*$ by introducing a control variable $c \in {0, 1}$
and factor $\phi_{c}$:
\[
\phi_{c}(b, c) = \left\{
  \begin{array}{l l}
    1, & \qquad \text{if } c = 0 \text{ and } b \in \{A_1,\dotsc,A_n\} \\
    1, & \qquad \text{if } c = 1 \text{ and } b \in \{A^*_1,\dotsc,A^*_n\}\\
    0, & \qquad \text{otherwise}
  \end{array} \right.
\]

By conditioning the control variable $c$ it is possible to obtain a graph with
potentials on any of the remaining variables equivalent to the potentials
obtained with $G_A$ (i.e.\ $c=0$) or $G_{U_a}$ (i.e.\ $c=1$). By introducing
an extra threshold factor $\phi_t(c)$ it also becomes possible to balance the
potential of the distribution described by $B$ such that a \gls{MAP} operation
on the $c$ variable decides on the novelty of $a$ with a desired threshold:

\begin{eqnarray*}
 P_B(c=0 | x)         &<& P_B(c=1 | x) \\
\Phi_B(c=0 \land x)   &<& \Phi_B(x=1 \land x) \\
\Phi_G(x) \phi_t(c=0) &<& \Phi_{U_a}(x) \phi_t(c=1) \\
P_G(x) Z_G \phi_t(c=0) &<& P_{U_a}(x) Z_{U_a} \phi_t(c=1) \\
\frac{P_G(x)}{P_{U_a}(x)} &<& \frac{\phi_t(c=1)}{\phi_t(c=0)}\frac{Z_{U_a}}{Z_G}
\end{eqnarray*}

Concluding, after merging graph $G$ and $U_a$ into graph $B$, the condition
described in \autoref{eq:threshold-with-ta} can be test by performing a
\gls{MAP} operation on variable $c$ of graph $B$ when the threshold factor
$\phi_t$ is proportional to:
\begin{equation}
\label{eq:structure-ratio}
\phi_t(c) \propto \left\{
  \begin{array}{l l}
    1, & \qquad \text{if } c = 0 \\
    t_A \frac{Z_G}{Z_{U_a}}, & \qquad \text{if } c = 1\\
  \end{array} \right.
\end{equation}

This equation is not very practical due to the need of calculation of
normalization factors $Z_G$ and $Z_{U_a}$. The presence of this factors
not only turns computation more expensive, but also requires that a full model
is known for all the factors disabling the ability to incorporate probabilistic
sensing output from complex detectors such as \glspl{SVM}. Ultimately, it also
removes capability to mix complex thresholds on several variables that would
allow to do probabilistic inference on the novelty of multiple variables at once.

 
\begin{figure}[h]
\centering
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=15pt, row sep=10pt,ampersand replacement=\&] {
    \node (thresholdguide) [latent,draw=none] {}; \&
    \&
    \node (room3) [latent] {}; \& \\
    \& \& \& \\
    \node (control) [latent,dashed] {$c$}; \&
    \node (room1) [latent,dashed] {$b$}; \& \&
    \node (room2) [latent] {}; \\
    \& \& \& \\
    \nofactor {shape1f}{}{$\phi_b$} {right=0pt}; \&
    \nofactor {appearance1f}{} {$\phi_b$} {right=0pt}; \&
    \nofactor {object1f}{} {$\phi_b$} {right=0pt}; \&
    \node (factor2) [factor] {}; \\
    \node (shape1l) [obs] {$S_p$}; \&
    \node (appearance1l) [obs] {$A_p$}; \&
    \node (object1l) [obs] {$O_p$}; \&
    \node (prop2) [obs] {$X_p$}; \\
  };
  \draw [-] (shape1f) -- (shape1l);
  \draw [-] (appearance1f) -- (appearance1l);
  \draw [-] (object1f) -- (object1l);
  \draw [-] (room2) -- (factor2) -- (prop2);
  \draw [-,draw=none] (room1) -- (room2) node (r12f) [midway,factor] {};
  \draw [-,draw=none] (room1) -- (room3) node (r13f) [midway,factor] {};
  \draw [-,draw=none] (room1) -- (control) node (controlf) [midway,factor] {};
  \draw [-] (room2) -- (room3) node (r23f) [midway,factor] {};
  \draw [-] (room2) -- (r12f);
  \draw [-] (room3) -- (r13f);
  \node (captr12f) [above=0pt of r12f] {\footnotesize{$\phi_b$}};
  \node (captr13f) [above=0pt of r13f] {\footnotesize{$\phi_b$}};

  \draw [-,dashed] (room1) -- (shape1f);
  \draw [-,dashed] (room1) -- (appearance1f);
  \draw [-,dashed] (room1) -- (object1f);
  \draw [-,dashed] (room1) -- (r12f);
  \draw [-,dashed] (room1) -- (r13f);

  \draw [-,draw=none] (room1) -- (control) node (controlf) [midway,factor] {};
  \draw [-,draw=none] (control) -- (thresholdguide) node (thresholdf) [midway,factor] {};
  \node (captcontrolf) [above=0pt of controlf] {\footnotesize{$\phi_c$}};
  \node (captthresholdf) [above=0pt of thresholdf] {\footnotesize{$\phi_t$}};
  \draw [-,dashed] (room1) -- (controlf) -- (control) -- (thresholdf);

  \begin{pgfonlayer}{background}
    \plate{places1}{(shape1f)(shape1l)(object1l)}{$\forall p \in places(room1)$}{};
    \plate{places2}{(factor2)(prop2)}{$\dots$}{};
  \end{pgfonlayer}
\end{tikzpicture}

\caption{\label{fig:merging-graphs}A condition based on the ratio between 2
         factor graphs that share some structure can be performed by testing the
         MAP probability of a variable on a merged graph.}
\end{figure}

\subsection{Graph Structure Influences $P(novel)$}
Analysing the reason for the normalization factors playing a role on the
threshold can be tracked down to the assumption that $P(novel)$ is
considered constant over any set or structure of $x$.
This requirement forces the threshold function to use the normalization factors
to compensate for a bigger likelihood of certain structure being naturally more
disposed to generate novel categories (an example of such structure is given on
\autoref{fig:structure-influences-novelty}).

A method to generate samples that match this \emph{criterium} of
having a constant $P(novel)$ can be:

\begin{algorithm}[Draw samples with equal $P(novel)$]
\begin{enumerate}
\item Sample a graph structure $G$ (where variable $a$ can be any of the known or unknown classes)
\item Decide novelty of $a$.
\item Sample the remaining variables according to distribution $P_G(x|a)$.
\end{enumerate}
\end{algorithm}


According to this sampling algorithm, given any structure the probability of
variable $a$ being novel is constant. This is somehow unexpected as it means
the sampling method is biased to adjust the real distribution $G$ such that
exactly a constant percentage of drawn samples are novel.

Alternative, the sampling method described below is considered more correct as
it introduces no bias on the distribution $G$ describing the real distribution
and classes of $a$:
\begin{algorithm}[Draw samples with equal $P(novel)$]
\begin{enumerate}
\item Sample a graph structure $G$ (where variable $a$ can be any of the known or unknown classes)
\item Sample variables $x$ according to distribution $P_G(x)$.
\end{enumerate}
\end{algorithm}

The next section shows how to generate a novelty detection method based on this
unbiased sampling method where samples are drawn directly from an assumed real
distribution aware of all the classes of $a$.

\begin{figure}[h]

\centering

\subfloat[Training Data]{
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=15pt, row sep=10pt,ampersand replacement=\&] {
    \node (a) [blue] {}; \& \node (e) [yellow] {}; \\
    \node (b) [yellow] {}; \& \node (f) [blue] {}; \\
    \node (c) [blue] {}; \& \node (d) [yellow] {}; \\
  };
  \draw [-] (a) -- (e) -- (f) -- (d) -- (c) -- (b) -- (f);
\end{tikzpicture}
\quad
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=15pt, row sep=10pt,ampersand replacement=\&] {
    \node (a) [yellow] {}; \& \node (e) [blue] {}; \\
    \node (empty) [blue,fill=none,draw=none] {}; \& \\
    \node (c) [blue] {}; \& \node (d) [yellow] {}; \\
  };
  \draw [-] (c) -- (a) -- (d) -- (e);
\end{tikzpicture}
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=15pt, row sep=10pt,ampersand replacement=\&] {
    \node (a) [yellow] {}; \& \node (e) [blue] {}; \\
    \node (b) [blue] {}; \& \node (f) [yellow] {}; \\
    \node (c) [yellow] {}; \& \node (d) [blue] {}; \\
  };
  \draw [-] (a) -- (e) -- (f) -- (d) -- (c) -- (b) -- (a);
\end{tikzpicture}
\quad
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=15pt, row sep=10pt,ampersand replacement=\&] {
    \node (a) [blue] {}; \& \node (e) [yellow] {}; \\
    \node (b) [yellow] {}; \& \node (f) [blue] {}; \\
    \node (c) [blue] {}; \& \node (d) [blue] {}; \\
  };
  \draw [-] (c) -- (b) -- (a) -- (e) -- (f) -- (b);
  \draw [-] (b) -- (d);
\end{tikzpicture}
\quad
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=15pt, row sep=10pt,ampersand replacement=\&] {
    \node (a) [yellow] {}; \& \node (e) [blue] {}; \\
    \node (b) [blue] {}; \& \node (f) [yellow] {}; \\
    \node (c) [yellow] {}; \& \node (d) [blue] {}; \\
  };
  \draw [-] (e) -- (a) -- (b) -- (c) -- (d);
  \draw [-] (b) -- (f);
\end{tikzpicture}

}

\subfloat[Sample A\label{fig:sample-a}]{
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=15pt, row sep=10pt,ampersand replacement=\&] {
    \node (a) [latent] {$a$}; \& \node (b) [latent] {}; \\
    \node (c) [latent] {};    \& \node (d) [latent] {}; \\
  };
  \draw [-] (a) -- (b) -- (d) -- (c) -- (a);
\end{tikzpicture}
}
\qquad
\subfloat[Sample B\label{fig:sample-b}]{
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=15pt, row sep=10pt,ampersand replacement=\&] {
    \node (a) [latent] {$a$}; \& \node (b) [latent] {}; \\
    \node (c) [latent] {};    \& \\
  };
  \draw [-] (a) -- (b) -- (c) -- (a);
\end{tikzpicture}
}

\caption{\label{fig:structure-influences-novelty}Given that the only known
         classes are yellow and blue, and that two nodes of the same class
         were never found connected together, it is reasonable to assume
         that on sample B the probability of node $a$
         be a novel class is greater than on sample A.}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Detecting Novel Classes on a Graph Variable}
On the previous section it was shown how to detect novelty on a variable
by assuming $P(novel)$ is constant across any structure. It was shown that
considering that \emph{criterium} a need to compensate the likelihood of
certain graph structures for generating novel classes arises and it requires
calculating normalization factors between the structures.
That \emph{criterium} is not only a nuisance but also represents
a different problem. It can be seen as an attempt to recognize that a sample
was drawn from a novel method instead of detecting whether a given variable on
a graph is a class that the system is not aware.

This section focus on the last one. It assumes there is a distribution modelled
by a graph $G$ that is used to drawn samples. On that graph $G$ there is a
variable $a \in \{A_1,\dotsc, A_K, A^*_{K+1},\dotsc, A^*_N\}$. The system is considered
to known only the states $\{A_1,\dotsc, A_K\}$. The goal is then: given a sample subset
$x$ of variables drawn from the graph, detect whether variable
$a \in \{A_1,\dotsc, A_K\}$.

That decision can be optimally modelled by the following condition:

\begin{eqnarray*}
P_G(a \in \{A_1,\dotsc, A_K\}|x) &<& P_G(a \in \{A^*_{K+1},\dotsc,A^*_N\}|x) \\
P_G(a \in \{A_1,\dotsc, A_K\} \land x) &<& P_G(a \in \{A^*_{K+1},\dotsc,A^*_N\} \land x) \\
2 P_G(a \in \{A_1,\dotsc, A_K\} \land x) &<& P_G(x)
\end{eqnarray*}

Similarly to the previous approach a graph $G'$ can be created based on $G$ such
that conditioning on a variable $c$ allows to obtain either the conditional or
unconditional probability of a given set of variables $x$.
For that variable $a$ is extended into $a'$ by duplicating the known states $A_i$
into $A^*_i$ and extending the factors associated with it.
Additionally a control factor $\phi_c(c, a)$ is added to make $c$ enforce the
desired constraints:

\begin{eqnarray*}
a' &\in& \{A_1, \dotsc, A_K,\quad A^*_1, \dots, A^*_K,\quad A^*_{K+1}, \dots, A^*_N\} \\
c &\in& \{0, 1\}
\end{eqnarray*}
\begin{eqnarray*}
\phi_c(c, a') &=& \left\{
  \begin{array}{l l}
    1, & \qquad \text{if } c = 0 \text{ and } a' \in \{A_1,\dotsc,A_K\} \\
    1, & \qquad \text{if } c = 1 \text{ and } a' \in \{A^*_1,\dotsc,A^*_N\}\\
    0, & \qquad \text{otherwise}
  \end{array} \right. \\
\end{eqnarray*}

Note that with this transformation, the following potential equivalences hold:
\begin{eqnarray*}
\Phi_{G'}(x, c=0) &=& \Phi_{G}(x | a \in \{A_1, \dots, A_K\})\\
\Phi_{G'}(x, c=1) &=& \Phi_{G}(x) 
\end{eqnarray*}


Although those potentials allow to determine novelty of $a$, they require
knowledge of all the possible classes, which is not available.
Nonetheless access to unlabelled data or other informations or assumptions on
the unconditional distribution allow to model or approximate unconditional
distributions of $a$. For that there is interest in trying to model the set
of states $\{A^*_1, \dots, A^*_N\}$ with a set of states $\{A^{\dag}_1, \dotsc, A^{\dag}_M\}$.

Moreover assuming that the set of states $A^\dag$ is able to perfectly
characterize the distribution over any graph $G$,
for a specific graph the potentials with states $A^\dag$ are a scale-factor
away from the potentials with states $A$. And that scale-factor is dependent on
the graph structure and factors of both $G$ and $G^\dag$:

\begin{eqnarray}
P_G(x) &=& P^\dag_G(x) \\
\Phi_G(x) &=& \frac{Z_G}{Z^\dag_G} \Phi^\dag_G(x)
\end{eqnarray}

Note that this ratio, although seems similar to the one obtained in
\autoref{eq:structure-ratio} it has a special property. Here both $G$ and
$G^\dag$ model the same probability, so it is expected that both scale they
normalization factor according to some relation on how a given factor differs
from $G$ to $G^\dag$.\footnote{On \autoref{eq:structure-ratio} the ratio is
between two different distributions: a conditional and an unconditional distribution.}

If on any graph structure the marginal probabilities of all variables except $a$
is the same either by using $A$ with $\phi$, or by using $A^\dag$ with $\phi^\dag$
then $\sum_{A^\dag}\phi^\dag_i(a^\dag, \cdot) = s_i \sum_A \phi_i(a, \cdot)$.
It can be proved that since the distribution over any graph structure has to be
equal, then the same holds for a graph structure $P_{G_i}$ composed by a single
factor $\phi_i$:

\begin{eqnarray*}
P^\dag_{G_i}(x) &=& P_{G_i}(x) \\
\frac{\phi_i(x)}{Z^\dag_{G_i}}  &=& \frac{s_i \phi_i(x)}{Z_{G_i}} \\
s_i &=& \frac{Z_{G_i}}{Z^\dag_{G_i}} \quad \text{(constant)}
\end{eqnarray*}

With this, the scale-factor that distinguish the potential on a graph
$G$ from the potential on $G^\dag$ is given by:

\begin{eqnarray*}
s &=& \frac{\Phi_G(x)}{\Phi^\dag_G(x)} \\
  &=& \frac{\sum_{a \in A}\prod \phi(a, x_\phi)}
           {\sum_{a \in A^\dag}\prod \phi^\dag(a^\dag, x_\phi)} \\
  &=& \frac{\sum_{a \in A}\prod \phi(a, x_\phi)}
           {\sum_{a \in A}\prod \phi(a, x_\phi) s_i} \\
  &=& \frac{1}{\prod s_i}
\end{eqnarray*}

If factor $\phi_i$ appears $k_i$ times on a graph structure then:
\begin{eqnarray}
\Phi^\dag_G(x) \prod {s_i}^{k_i} = \Phi_G(x)
\end{eqnarray}

Alternative, if factors $\phi^\ddag_i(a^\dag,x) = \phi^\dag_i(a^\dag,x)/s_i$ are
introduced then:
\begin{eqnarray}
\Phi^\ddag_G(x) = \Phi_G(x)
\end{eqnarray}

Having shown how to replace a variable $a$ for an equivalent $a^\dag$, and
respectively change the factors dependent on $a$ from $\phi_i(a,\cdot)$ to
$\phi^\ddag_i(a^\dag,\cdot)$ such that the potentials on the original
graph are kept constant it becomes possible to go back to the graph $G'$ and
replace the states $\{A^*_1,\dotsc,A^*_N\}$ for the equivalents $A^\ddag$
and change the factors associated with $a'$ for:

\begin{eqnarray*}
\phi_i(a', \cdot) &=& \left\{
  \begin{array}{l l}
    \phi_i(a',\cdot), & \qquad \text{if } a' \in \{A_1,\dotsc,A_K\} \\
    \phi^\ddag_i(a', \cdot), & \qquad \text{if } a' \in A^\ddag
  \end{array} \right. \\
\end{eqnarray*}

It is now possible to perform a decision on the novelty of $a$ with the condition:
\begin{equation}
2 \Phi_{G'}(x | c = 0) < \Phi_{G'}(x | c= 1)
\end{equation}

This condition is particularly useful since it does not depends on the normalization
factors of $G'$. This allows to use \emph{intractable models} for some factors,
e.g.\ using an \gls{SVM} for factors associated with sensed variables. 


\subsection{Estimating the Factor Scaling-factors $s_i$}
On the previous section it was shown how to detect novelty on a variable $a$ by
approximating the unconditional distribution factors with factors $\phi^\dag_i$
that are a scalar $s_i$ away from the ones used on the real distribution.

This section address the problem of estimating those parameters.




\subsection{Old content}
To sum up, the method described above allows to integrate novelty detection
into the graph, providing the agent with tools to describe its belief on the
novelty of variables on the system and to continue working with such a
representation. The mechanism is also fully probabilistic and at no time
the agent needs to take an irrevocable decision on the novelty of a variable,
allowing the agent beliefs to change over time as more data becomes available.


\subsection{Handling novelty signals from the low-level classifiers}
On \autoref{sec:clues-from-low-level} it was shown that information from the
low-level classifiers is integrated in the conceptual map in a probabilistic
way by introducing a factor $\phi_o$ on each observed variable that describes
the probabilistic distribution of that variable given by the classifier.

Since in order to detect novelty all the variables have been extended with a
new state indicating novelty. That same state can be used to integrate novelty
signals coming from the low-level classifiers.

TODO: maybe describe how those novelty signals could be generated from the
low-level classifiers\dots decision boundary distance, kernel pca, etc\dots

\section{Detecting New Structure}
I do not belive there is any time left for this.
