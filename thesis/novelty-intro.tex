\chapter{Novelty Detection}\label{chap:novelty-intro}

This chapter presents the main work of this thesis: a method to augment the conceptual
map with novelty detection capabilities.

The chapter starts by providing the reader with a brief overview on novelty detection
techniques and related works. Then it shows that an optimal novelty detection system
can be implemented by thresholding on a order-relation defined over the inputs.
And that an equivalent order-relation can be imposed by a ratio between
conditional and unconditional probabilities.
Some discussion on how to interpret the meaning of those factors is also
presented.

At last a practical example using semantic data and probabilistic graphical
models is presented: it shows how to use the introduced ratio to obtain a
novelty detection system and analyses the performance impact by increasing
the amount of sensed data and by using an approximation on the unconditional
probability.


\section{Novelty Detection Review and Related Work}
Novelty detection, also known as outlier or anomaly detection, is a
classification problem related to identification of new or unknown data
patterns that the system is not aware of~\cite{markou2003novelty}.

The ability to identify novel cases is crucial in any autonomous system
that is deployed to unknown or to uncontrolled environments, as it gives the
system the ability to detect that something is not conforming to its knowledge and
therefore should be treated with caution.
It has several applications such as fault detection~\cite{tarassenko1999novelty},
intrusion detection~\cite{fan2001using},
detection of masses in mammograms~\cite{tarassenko1995novelty} or detection of
novel and useful documents~\cite{zhang2002novelty}.

Any normal classification application is also a good candidate for extending
with novelty detection, as the results for samples the system has not been
trained on can be unreliable~\cite{devarakota2008reliability}.
e.g.\ applied to digit-recognition~\cite{tax1998outlier}
or to detection of novel inputs on a neural network~\cite{bishop1994novelty}.

It is in the nature of unknown environments and in the infeasibility of
the system to acquire examples of all possible classes where the complexity of
novelty detection lies: it is only possible to obtain samples representing positives
examples of known cases and the lack of negative examples renders normal
classification methods unusable.
In order to become practical in real world applications, novelty detection
methods have to overcome a series of obstacles:
be able to generalize while still detecting novelty,
be resistant to noisy features,
ability to scale in feature dimension,
deal with multiple classes and performing detection efficiently as many
autonomous systems require real-time or close to real-time performance.

\subsection{Review}
Many approaches have been made based on statistical methods. Those often model
the training data with statistic distributions and then perform hypothesis tests.
For example: testing whether two distributions are the same or using distance to
mean value or quartiles.
In some sense all they define a distance measure to normality and employ a
threshold on that.

A common approach is to use density estimation and use the expected probability
of a sample to trigger the sample as novel. Examples are usage of
Gaussian Mixture Models and Parzen-window estimators. In order to be effective
those use data as close as possible to the input features and use
dimension-reduction techniques such as \gls{PCA} to make density
estimation feasible. Note that as dimension increases an exponential number
of data samples would be required to approach the density with the same quality.

\cite{bishop1994novelty} uses that approach by employing a Parzen-window to
estimate the density of the training data on a given input feed into a
neural-network. By triggering on the calculated density they detect samples
that differ from the training data and consider their neural-network output
to be unreliable as the samples are distinct from what the network was trained
with.

A slightly different approach is done by one-class \gls{SVM} approaches that
try to distinguish novelty by separating the training set from all the other
points in the input space. They try to achieve that by enclosing the training
set by some structure (e.g.\ an hyper-sphere)~\cite{bennett2000support}.
Those approaches have been made in line with Vapnik principle of not solving
something hard. As although having access to a perfect probability distribution
of the input would solve the problem, creating such a function is harder than
simply creating a boundary between known data and novel data
\cite{scholkopf2000support}.

Error reconstruction methods have also been used for novelty detection.
They use the assumption that the class to be defined lies on a manifold embedded
on an input space of higher dimensions. By using dimensional reduction
techniques, they try to defined that manifold and calculate how far a new sample
is from it.
One of the most common methods used for that is
\gls{K-PCA}~\cite{scholkopf1997kernel}, which uses the kernel-trick to extend
\gls{PCA} and perform a nonlinear dimensionality reduction on the input.
This technique has been successfully used in novelty detection by
\cite{Hoffmann2007863}.

\subsection{Related Work}
\cite{zhang2005probabilistic} present a probabilistic semantic approach to image
annotation and retrieval with a multi-modal information. They introduce an
hidden variable modelling a concept present on a scene that connects together
information coming from both a bag of words and visual features extracted. By
training that hidden-variable their system becomes aware of possible existent
semantic categories and their relation between both textual and visual features.
Although not able to perform novelty detection, their approach is interesting by
the use of an hidden variable to probabilistic model the multi-modal sensors.

\cite{boutell2006factor} presents a method to perform scene-classification
from low-level region detectors using factor graphs. Those low-level detectors
provide semantic (high-level) cues on the higher level scene-classification
task. Due to the nature of the low-level detectors, they ought to be treated
in a probabilistic way. For that their method uses factor graphs. The method
also exploits spatial relation information between regions to improve the
classification performance. A presented scheme using only pairwise relations
between regions is show to have better performance than try to approach the
correct scheme with a big single-factor. I.e.\ the pairwise relations can be
approximated with the modest amounts of training data, where the single-factor
demands an impractical amount of training data.
No approach is made on it at novelty detection either of regions or scene
categories. 

\cite{bishop1994novelty} works shows that assuming a uniform unconditional
distribution novelty detection can be performed by triggering on the
density probability of the training data. The system is employed to detect novel
inputs for a neural-network.

\cite{ranganathan2010pliss} presents a system able to perform place recognition
and classification from visual clues. It is able to perform segmentation by
exploiting time-coherency on video information. The approach is particularly
interesting by its ability of keeping a fully probabilistic distribution of
the place classification and segmentation. This is, the system never performs
a deterministic decision that impacts any future result, allowing it to adjust
the segmentation and place classification as more data becomes available.
The system is also able to detect novel instances and methods how to adapt it
to run on a constant amount of memory and computation are presented.

\section{Novelty Detection by Thresholding}
The objective of a novelty detection system is to classify a given sample $x$ as
either \emph{known}: $x$ being generated by a known class to the system, or \emph{novel}:
$x$ generated by a class unknown for the system.
Based on the ground\hyp{}truth of a sample four cases are possible:

\begin{description}
\item[true positive]  - when a system correctly   flags a sample of an unknown class as \emph{novel}.
\item[false positive] - when a system incorrectly flags a sample of a known class as \emph{novel}.
\item[true negative]  - when a system correctly   flags a sample of a known class as \emph{known}.
\item[false negative] - when a system incorrectly flags a sample of an unknown class as \emph{known}.
\end{description}

Due to noisy data, unstable features and lack of information it is impossible to develop
a method able to exactly guess the correct classification of a sample.
By modelling the outcomes with probabilities it becomes possible to handle the uncertainty
associated with each decision.
The notation $P(novel|x)$ will be used to denote the probability that the sample $x$ is in fact a
sample of an unknown class. $P(x)$ will denote the probability that sample $x$ is given to the
system to be classified. $\overline{novel}$ is also defined in such a way that
$P(\overline{novel}|x)$ measures the probability that a decision on classifying $x$ as novel is
wrong.

Additionally a decision on the novelty of a sample $x$ performed by a deterministic system
will be fully determined by the sample itself.
Which implies that any deterministic novelty detection system can be uniquely determined by the set $N$
of samples that are accepted by the classifier as \emph{novel}.
This way it is possible to define the probability of a true positive and false positive event for any
deterministic novelty detector with the following equations:

\begin{eqnarray}
P(\text{true positive})  &=& \sum_{x \in N}{P(novel|x)P(x)} \\
P(\text{false positive}) &=& \sum_{x \in N}{P(\overline{novel}|x)P(x})
\end{eqnarray}

Note that since probability functions are non-negative, it is impossible to decrease either the
true positive or the false positive probability by using a set $N' \supset N$.
This describes the base of the \emph{error and rejection tradeoff}~\cite{chow1970optimum}, which
states that a system aiming at increasing the true-positive probability will eventually increase its
false-positive error.
The true positive probability can be described as the interest in detecting as much
as possible the novel classes and the false positive probability the interest
in not incurring in too many errors.
By fixating one of those it is possible to define a novelty detection
system that achieves the maximum or minimum of the other.

This way an optimal detector can be formulated by achieving the maximum true-positive
probability without its false-positive probability increase beyond a given limit.
This is equivalent to a \emph{continuous knapsack problem} which allows a greedy
solution by sorting the items with a value per weight function.
In the case of detection system that can be defined as:

\begin{eqnarray}
value(x)  &=& P(\text{true positive}|x) \\
weight(x) &=& P(\text{false positive}|x) \\
cost(x)   &=& value(x)/cost(x) \\
          &=& \frac{P(novel|x)P(x)}{P(\overline{novel}|x)P(x)}
\end{eqnarray}

Therefore a novelty system before classifying a sample $a$ as novel should (greedily)
classify any sample $b$ with a small cost as that would achieve
an higher true positive probability given a fixed false positive one.

\begin{equation}
\label{eq:knapsack}
\frac{P(novel|b)}{P(\overline{novel}|b)} < \frac{P(novel|a)}{P(\overline{novel}|a)}
\end{equation}

This relation between $a$ and $b$ can further be simplified into:

\begin{equation}
P(\overline{novel}|b) < P(\overline{novel}|a)
\end{equation}


Based on this, it can be said that an optimal novelty detection system is
interested in defining an order relation on all the possible inputs equivalent
to the order defined by the function: $P(\overline{novel}|x)$.
And any optimal detector can be described by the largest $P(\overline{novel}|x)$
accepted by it. Which is seen as threshold.


\section{Conditional and Unconditional Probability Ratio}

On the previous section it was shown that an optimal novelty detector can be
implemented with a threshold on top of the order-relation defined by
$P(\overline{novel}|x)$ over $x$. Performing some manipulations with
Bayes theorem and assuming a constant $P(\overline{novel})$ a more usable
form can be attained:

\begin{equation}
\label{eq:novelty-ratio}
          P(\overline{novel}|x)
  =       \frac{P(x|\overline{novel}) P(\overline{novel})}{P(x)}
  \propto \frac{P(x|\overline{novel})}{P(x)}
\end{equation}

Since there is only interest in maintaining the same order-relation as
$P(\overline{novel}|x)$ any constant factor can be dropped.
Leaving a ratio between a \emph{conditional} and
\emph{unconditional probability} suitable for implementing novelty detection
by thresholding.


\subsection{Conditional Probability}
The conditional probability $P(x|\overline{novel})$ describes the distribution
of the samples given that they are generated by a known class.
In case the labelled data available for the agent to learn a concept comes from
the same distribution where the system will run the correct approach is to use it
as prior-information for modelling the conditional probability.

Note that it is important for the labelled data to be a filtered version
of the underlying world distribution (with all classes) that does not contains
any bias.
Otherwise that bias will lead to incorrect modelling the conditional probability
and a subsequent wrong ordering of the sample space.

\subsection{Unconditional Probability}
The unconditional probability $P(x)$ plays an important role on obtaining a
correct order relation for performing novelty detection.
It serves as a normalizing component that allows the system to figure out
whether a given sample conditional probability arises from it belonging to
the known concept or from the likelihood of being sampled.

On lack of any information about the unconditional probability and conforming to
the principle of maximum entropy (\autoref{sec:max-entropy}) a uniform
distribution must be chosen.
Though by using unlabelled data it becomes possible to obtain extra information
and achieve a better approximation.


%\subsection{Novelty Detection on a Variable Set of Features}
Note also that often novelty detection is applied on a fixed set of features
together with an assumption of a uniform unconditional probability.
On those cases $P(x)$ becomes a constant and therefore a novelty threshold
can be directly applied on $P(x|\overline{novel})$ as is the case in \cite{bishop1994novelty}.
But in the case where the set of features $x$ is variable it cannot be
discarded. There $P(x)$ also plays a role in levering all the conditional
probabilities on different sets of variables into the same measure units
such that a threshold can be implemented.


\section{A Practical Example}
\label{sec:unlabelled-data}
In order to sum up the presented concepts on novelty detection with a threshold
function and exemplify how to use graphical models in the context of
multi-modality room classification a synthetic dataset was generated.
The dataset was kept simple by only modelling directly sensed features from a
room, skipping any structural knowledge or sensing model such as room
connectivity and extra hidden variables.

In this dataset a room $r$ is seen as an hidden-variable generator of a set of
features $X$ that are directly sensed by the agent.
All the sensed features $x$ are independent given the room category.
In whole there was 11 different room categories and 7 different feature types.
Each feature can be sensed more than once (e.g.\ room shape is extracted from 2D
laser scans in more than one position in the room), but all those sensed
instances are considered independent given the room category.

The room categories were chosen to mimic as close as possible the real features
and categories existing in reality (i.e.\ 1 person office, 2 person office,
hallway, robot lab, etc\dots). A table describing the used synthetic
distribution is given in \autoref{extra:synthetic-distribution}.

The objective then was to design a system that, although only trained with
labelled data from 5 of the 11 room categories, was able to detect novel
room categories.
For that 100 labelled samples for the 5 known categories were drawn and 1000 
unlabelled samples were drawn from all the room categories for learning the
unconditional probability distribution and measure effect of using unlabelled
data.

\subsection{Conditional Probability}
Using the labelled samples, 7 factors $\phi_X(r,x)$ were created, one for each
feature type, to represent the potential of sensing features $x$ of type $X$ on
a room of category $r$.

% TODO: discuss this with Andrzej
\begin{equation}
\phi_X(r,x) = \frac{\#(r,x)+C}{\sum_{i \in X}{\#(r,i)+C}}
\end{equation}

Where $\#(r,x)$ denotes the number of times a feature $x$ was sensed inside a
room category $r$ and $C$ is a smoothing parameter that accounts for fixing
the probabilities in case a given sample is never seen.
With those the probability of sensing a set of features $x$ on a room category
$r$ known for the agent can be modelled with a factor graph as illustrated on
\autoref{fig:simple-cond-graph}.

\begin{figure}[h]
\centering
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=20pt, row sep=10pt,ampersand replacement=\&] {
    \& \& \node (room) [latent] {$r$}; \& \& \\
    \& \& \& \& \\
    \node (f1) [factor] {}; \&
    \node (f2) [factor] {}; \&
    \node (f3) [factor] {}; \&
    \node (fi) [] {\dots}; \&
    \node (fn) [factor] {}; \\
    \node (x1) [obs] {$x_1$}; \&
    \node (x2) [obs] {$x_2$}; \&
    \node (x3) [obs] {$x_3$}; \&
    \node (xi) [] {\dots}; \&
    \node (xn) [obs] {$x_n$}; \\
  };
  \draw [-] (room) -- (f1) -- (x1);
  \draw [-] (room) -- (f2) -- (x2);
  \draw [-] (room) -- (f3) -- (x3);
  \draw [-] (room) -- (fi);
  \draw [-] (room) -- (fn) -- (xn);

  \node (captf1) [right=2pt of f1] {\footnotesize{$\phi_{X_1}$}};
  \node (captf2) [right=2pt of f2] {\footnotesize{$\phi_{X_2}$}};
  \node (captf3) [right=2pt of f3] {\footnotesize{$\phi_{X_3}$}};
  \node (captfn) [right=2pt of fn] {\footnotesize{$\phi_{X_n}$}};
\end{tikzpicture}
\caption{\label{fig:simple-cond-graph}A factor graph modelling the conditional
probability of sensing a set of features $x$ given that the room category $r$ is
one of the known classes.}
\end{figure}

\subsection{Unconditional Probability}
\label{sec:sample-uncond-prob}
With no knowledge on the unconditional probability the correct approach is to
assume a uniform distribution.
That is represented in factor graphs by a graph without any factors.

\begin{figure}[h]
\centering
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=20pt, row sep=10pt,ampersand replacement=\&] {
    \node (x1) [obs] {$x_1$}; \&
    \node (x2) [obs] {$x_2$}; \&
    \node (x3) [obs] {$x_3$}; \&
    \node (xi) [] {\dots}; \&
    \node (xn) [obs] {$x_n$}; \\
  };
\end{tikzpicture}
\caption{\label{fig:simple-uniform-graph}A factor graph modelling a uniform
         distribution over the sensed set of features $x$.}
\end{figure}

Very often there is extra knowledge that can be obtained about the distribution
of the variable that helps to model the unconditional distribution.
In this practical example the access to unlabelled data is explored as access to
it is common on applications in robotics.

Note that the sensed variables are dependent between each other when the room
category $r$ is not known. With that the correct approach would be to train a
factor that is able to correlate all the sensed variables as seen on
\autoref{fig:simple-all-dependent}.

\begin{figure}[h]
\centering
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=20pt, row sep=10pt,ampersand replacement=\&] {
    \& \& \node (fac) [factor] {}; \& \& \\
    \& \& \& \& \\
    \node (x1) [obs] {$x_1$}; \&
    \node (x2) [obs] {$x_2$}; \&
    \node (x3) [obs] {$x_3$}; \&
    \node (xi) [] {\dots}; \&
    \node (xn) [obs] {$x_n$}; \\
  };
  \draw [-] (fac) -- (x1);
  \draw [-] (fac) -- (x2);
  \draw [-] (fac) -- (x3);
  \draw [-] (fac) -- (xn);
  \draw [-] (fac) -- (xi);
\end{tikzpicture}
\caption{\label{fig:simple-all-dependent}A general factor graph able
         to model any unconditional distribution on the sensed variables
         requires a factor connecting all of them.}
\end{figure}

Nonetheless such an approach suffers from
the \emph{curse of dimensionality}: as the number of sensed features and feature
types increases exponential amounts of data are needed to describe it.
Having interest in avoiding the issues associated with an high-connected factor
an assumption on independent features can be done and then the unconditional
probability can be modelled with a fully disconnected variables, but with
factors that account for existent bias on each single feature (as visible on
graph in \autoref{fig:simple-independent-graph}).

In the case there is only one sensed feature the distribution generated by an
assumption of independent features correctly models the unconditional
probability. And it deviates from it as more features are sensed.
The individual factors associated with each variable can be seen as a scaling of
each feature that tries to account for a possible existent bias on each
of them. Therefore, although far from the desired factor, it is expected to be a
better estimate than assuming a uniform distribution.

\begin{figure}[h]
\centering
\begin{tikzpicture}
  \node [matrix,matrix anchor=mid, column sep=20pt, row sep=10pt,ampersand replacement=\&] {
    \& \& \& \& \\
    \node (f1) [factor] {}; \&
    \node (f2) [factor] {}; \&
    \node (f3) [factor] {}; \&
    \node (fi) [] {\dots}; \&
    \node (fn) [factor] {}; \\
    \node (x1) [obs] {$x_1$}; \&
    \node (x2) [obs] {$x_2$}; \&
    \node (x3) [obs] {$x_3$}; \&
    \node (xi) [] {\dots}; \&
    \node (xn) [obs] {$x_n$}; \\
  };
  \draw [-] (f1) -- (x1);
  \draw [-] (f2) -- (x2);
  \draw [-] (f3) -- (x3);
  \draw [-] (fi);
  \draw [-] (fn) -- (xn);
\end{tikzpicture}
\caption{\label{fig:simple-independent-graph}A factor graph modelling an
         independent distribution over the sensed set of features $x$.}
\end{figure}



\subsection{Threshold Functions}
Three threshold functions were created using the knowledge on the synthetic
distribution and the models learnt from the sample data:
$G$ on \autoref{fig:simple-cond-graph},
$U$ on \autoref{fig:simple-uniform-graph} and
$I$ on \autoref{fig:simple-independent-graph}.

\begin{description}
\item[exact] 
- since the distribution is synthetic there is access to $P(x)$ and $P(x|concept)$
and the ordering function $P(x|\overline{novel})/P(x)$ could be created
to test how far the other presented thresholds are from optimal.


\item[uniform]
- by assuming a uniform unconditional distribution the ordering function is
  given by $P_G(x)/P_U(x)$.

\item[independent]
- using the unlabelled data the unconditional distribution can be approximated.
  In this case, it was approximated with an independent distribution of the
  sensed features. The independent threshold was implemented with $P_G(x)/P_I(x)$.
\end{description}


\subsection{Probability Ratio Comparison}
%%% Results 1
% Show the threshold ratio is an optimal detector (if perfect information was available)
% Show that the thresholds are suitable functions for implementing a static threshold.
As first tryout, the performance of the novelty threshold selection was plotted for a set
of 1000 samples taken out from the whole distribution (\autoref{fig:synthetic-roc}).
Those samples contain different number of sensed properties for each room, mimicking
the dynamic properties expected to see when implemented on a robot.

\begin{figure}[h]
\centering
\includegraphics[width=0.60\textwidth]{results/synthetic-all.pdf}

\caption{\label{fig:synthetic-roc}ROC curve comparing novelty detection performance
         under samples with variable size of sensed properties.}
\end{figure}

The convex shape for the optimal threshold shows that the ratio between conditional
and unconditional probability is indeed an optimal detector and is suitable for
implementing a threshold when the samples are taken from dynamic
distributions (e.g.\ some samples where there is only access to room size versus
samples where there is a lot of information about the room properties).

% Discuss importance on approximating unconditional probability.
Its also possible to see how important it is to estimate a correct unconditional
probability in order to obtain a correct novelty measure on the inputs.
The assumption of a uniform unconditional probability has led to very poor results.
That is probably explained by the semantic properties being highly
biased towards some values. And shows that bias plays an important step
in detecting whether a given sensed value is a valuable cue about the room category.



%%% Results 2
% Measure performance of the thresholds as more information becomes available.
\subsection{Performance Changes With Amount of Available Information}
In order to measure the performance impact as more semantic information becomes
available ROC curves were plotted for samples grouped by the number of sensed
semantic features.

\begin{figure}[h]
\centering

\subfloat[3 sensed features]{\includegraphics[width=0.40\textwidth]{results/synthetic-3features.pdf}}
\qquad
\subfloat[5 sensed features]{\includegraphics[width=0.40\textwidth]{results/synthetic-5features.pdf}}

\subfloat[10 sensed features]{\includegraphics[width=0.40\textwidth]{results/synthetic-10features.pdf}}
\qquad
\subfloat[50 sensed features]{\includegraphics[width=0.40\textwidth]{results/synthetic-50features.pdf}}

\caption{\label{fig:synthetic-roc-breakdown}ROC curves plotted showing performance of the
         presented novelty detection method on graphs generated for different amount of
         sensed features.}
\end{figure}

It is possible to see that as the system gains more semantic information it
becomes easier to detect novelty. The input space size increases and allows the
several existing classes to become more easily distinguished.

The performance of the independent threshold decreases as the number of sensed
features increases. This is easily explained by the fact that the graph $I$ is not
able to model the existent dependence between the features. This becomes obvious
as the number of features increases (e.g.\ graph $I$ perfectly models $P(x)$ in the
case where only 1 feature is sensed).

The uniform threshold shows a poor performance specially on small size samples
where its performs almost no better than random.
It performance increases as the size of sensed features increases but nonetheless
its very small when compared to how optimal a threshold could be.

