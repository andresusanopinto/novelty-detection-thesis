% What should the title be?
% Novelty Detection for Visual Indoor Categorization
% Novelty Detection on Semantic Representations.


\chapter{Introduction}
% Introduce:
%  Mobile Robotics, A.I.
%  Interaction with Humans
%  Need for concepts/semantic information
%  Dynamic human environment
%  Non-feasibility of hard-coded concepts
%  Requirements to detect novelty and handle it.
For a long time humanity has fantasized that one day robots will walk among us.
They will move and be able to interact with us. Understand our concepts and
be able to reason.

They need to possess ability to adapt to situations as its infeasible to rely
on extensive man-work to tag objects, map space and code all the aspects that
make up our human-reality.

% == Introduce the need of semantic representations ==
%
% There is a lot of low-level methods (or specific knowledge to robots)
% But the creation of semantic representations allow to bridge that low-level
% sensing with high-level concepts that facilitate several high-level Tasks.


% Importance of mapping and localization in a robot as well as reasoning on
% properties of each space.
% Humans give labels to space that characterized the properties and activities
% expected to be performed on them.
% Such information is of great interest to be structured and organized in such
% a way reasoning and planning can be implemented.
%
% It also holds an important role in long-term planning and stability as it
% hides the space-time local details and allows to focus on high-level thinking.

% Deal with uncertainty -> need for probabilistic models.
% Deal with semantic    -> need of structured models such as graphical models.


Example of what kind of an high-level thinking on space and its semantics can
provide: Get me a beer/milk concept.

% == Reasoning about need for Novelty Detection ==
%
% Dynamic environment (inability to know the environment or even to be possible
% to map all semantic concepts to interact with humans).
%
% Knowledge-Awareness (knowing what we know... And detecting novel cases)
% Identify gaps in the semantic knowledge.
% Automatic detection and learning of novel concepts.
%
% Increase robustness
% Self-Extending
%


\section{Problem and Goals}
% Extend a probabilistic graphical modelling framework with capabilities to
% detect novelty.
% Both in terms of novel classes as of novel structure.
%
Given a probabilistic structure representing the sensed conceptual knowledge
obtained by the agent develop methods to be able to detect novelty present
either on new semantic concepts (new classes) or even on structure that was
previously unknown.


% Yeah thats what you call dream :P
In an extreme ideal case the agent should be able to go from zero-knowledge to
understanding presence of certain objects (as generators of a set of sensed
sensed properties grouped locally), understand areas within rooms (sink-area on
on a kitchen), rooms as separated by doors, and understand environments such
as office, home, warehouse, spaceship.


For that the probabilistic semantic representation presented by
\cite{andrzej2011phd} is used.

% If everything goes fine... 2 of the "Future Directions" proposed are 
% Novelty Detection and Learning of Novel Concepts:
%  Identify Gaps in Spatial and Semantic Knowledge -> Addressed
%  Performing learning of new concepts -> Outside Scope / No Time
%
% Using Properties for Space Segmentation
%  If we move on detecting novel structures on the graph we are addressing this
%  issue.



\section{Thesis Outline}
The rest of this thesis is structured as follows:

\autoref{chap:background} introduces background concepts for understanding the
presented thesis. In particular \autoref{sec:graphical-models} introduces
\emph{probabilistic graphical models} that lay the base modelling tool for
the used structured semantic representation.

\autoref{chap:semantic-mapping} describes the system proposed by
\cite{andrzej}. In special it introduces the \emph{conceptual map} of the system
that is responsible for the creation of the \emph{probabilistic semantic
representation} that this thesis aims at improving by developing methods with
the capability to identify knowledge gaps.

\autoref{chap:novelty-intro} introduces novelty detection under a
statistical view point and how to interpret it as a threshold function.
A simple approach on how to perform novelty on very simple graphs
is given together with some results on the impact of using semi-supervised
novelty detection to improve the system performance.

\autoref{chap:novelty} presents the developed techniques developed to identify
novelty on the semantic representation.

\autoref{chap:conclusions} draws conclusions on the developed work and presents
interesting directions for future works.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}\label{chap:background}
% This chapter will perform a breath-first explanation on the contents related
% with the thesis. The extension of the contents explained here can be seen as
% monotone function over the number of missing pages to attain the requirements.
%
% Some topics are indeed important as its the case of factor graphs.
% Others like:
%  - classification (SVM, multi-class, kernel-trick),
%  - features (SIFT, GIST),
%  - Basic Bayes theory and probabilistic review
% are included for fun of the reader in case it inquires himself how to deal
% with the low-levels features we propose using.
%
% ---
% PS: This is the chapter that will be used to fill with unnecessary crap
% in case they really annoy me with the number of pages. Every other chapter
% will strive to be really required and be a masterpiece.
%

\section{Classification}
\subsection{Recognition and Categorization}
\subsection{Support Vector Machines}
\Glspl{SVM} where introduced by \cite{cortes1995support} and can be seen as
discriminative linear based classifier. They are based on a strong mathematical
foundation and have powerful generalization capabilities. In their original form
\gls{SVM} separates two classes of points in an hyper-space with a
\emph{maximal margin hyperplane}~(\autoref{fig:svm-sample}).

Later they were extended to deal with noisy data by using \emph{soft margins}.
And to handle non-linear spaces as seen on \autoref{sec:kernel-trick}.
Although they only allow for two class-classification several methods have been
proposed to build multi-class classifiers based on binary classifiers as seen on
\autoref{sec:multiclass-classifiers}.

\begin{figure}[h]
\begin{center}
% TODO: get a decent picture to illustrate SVMs
\includegraphics[width=0.4\textwidth]{figures/Svm_max_sep_hyperplane_with_margin}
\end{center}
\caption{{SVM} separating two class of points by a
         \emph{maximal margin hyperplane}. The hyperplane can be described by
         the collection of support vectors and associated weights, marked in the
         image as sample points with large borders.}
\label{fig:svm-sample}
\end{figure}

They have been used in several classification and recognition problems and are
in fact a standard across machine learning techniques. Their efficiency, exact
training results and generalization made their suitable for many tasks. Such as
text categorization, digit-recognition, spam-classification.
They have also been extensively used in visual place classification.


\subsection{Multi-Class Classification}
\label{sec:multiclass-classifiers}
\Glspl{SVM} were designed as two-class classifier. And so they need to be
adapted for using in multi-class problems. The most common usage is to design
multi-class classifiers recurring at the usage of multiple two-class classifiers
and methods for combining them. The most typical approaches for a multi-class
classification on $c$ classes using \glspl{SVM} are:

\begin{description}
\item[One Against All] - in this method $c$ distinct classifiers are trained to
distinguish any class of the remaining ones. The output of all those classifiers
(distance to the separating hyperplane) is then used to categorize the output.
The most common approach is to pick the class with the largest hyperplane
distance. Other variations exists as is the case of using the minimal distance
to the average classification distance of each class~\citep{pronobis2007confidence}.

\item[One Against One] - in this method $c*(c-1)/2$ classifiers are trained to
distinguish between each pair of classes. The final decision is based on the
output of all those classifiers being common to use a majority vote strategy.
\end{description}

\subsection{Kernel-Trick}
\label{sec:kernel-trick}
\gls{SVM} in their basic form are only able to handle linear spaces.
Nonetheless the classes are most of the time not linearly separable in the input
space. Although there might exists a transformation $\phi$ from the original
space into a space $H$ where the input becomes linearly separable.

The Kernel trick allows to extend the \gls{SVM} definition to work on such space
$H$ without ever performing an implicit transformation between spaces. Being
enough for that to have a Kernel function defining an inner-product inside such
space: $K(x_i, x_j) = \phi(x_i)\cdot\phi(x_j)$.
In that sense a Kernel function can be seen as a function that calculates some
similarity measure between two inputs.

Several kernel functions have been proposed being the most commonly used:

\begin{description}
\item[Polynomial Kernel] - $K(x, y) = (x \cdot y + p)^d$
\item[Radial Basis Function] - $K(x, y) = e^{-\gamma\|x - y \|^2}$
\item[Histogram intersection] - $K(x, y) = e^{-\gamma \chi^2(x,y)}$, where
$\chi^2(x,y) = \sum_{i=1}^{N}\frac{(x_i-y_i)}{x_i+y_i}$ introduced by
\cite{barla2003histogram} allows to compute histogram similarity.
\item[Matching Kernels] - mimic matching similarity and are used when each
sample is represented as a set of features~\citep{boughorbel2005intermediate}.
\end{description}

\section{Features}
A feature is a piece of information which is expected to reveal information for
solving a specific task. In that sense features are task-dependant and they will
yield different performance based on the type of task they are applied to.

A wanted property on features is its repeatability under similar conditions for
the problem in hand. This is: they should be stable and invariant across
unwanted types of transformations and noise. For example a visual feature for
object detection should be present even if the target object was translated,
scaled, rotated, the light-conditions have changed or even if the object is
partially occluded.

Extracting features with those properties allows to greatly reduce the size of
input by removing unwanted noise and useless information from the captured data.
Turning the classification problem easier, more reliable and more efficient.

Often several and different types of features need to be extracted. It has been
reported by \cite{pronobis2010ijrr} that using multiple features provides a
great benefit in the context of place classification.
And \cite{quattoni2009recognizing} has showed that different types have
different impact in indoor scene recognition based on the type of scene
matching. Namely was seen that some room-categories are more likely to be
recognized by the presence of some objects and others by it generic appearance.

In the context of robotics, sensors such as cameras, laser scans are used to
sense the surrounding environment, and features can be extracted from all those.
Nonetheless as described in \autoref{sec:visual-motivation} the visual sensor is
incredibly rich and most of our features will come from it.

Visual features can be seen as belonging to two categories:
\emph{local features} and \emph{global features}.
\emph{Local features} describe fine grain properties of a part of image.
For example the existence of specific corner or an edge. \Gls{SIFT}
(\autoref{sec:sift}) is an example of such feature and has been proven useful
for matching points between images and subsequence extension to object detection.

\emph{Global features} such as \gls{CRFH} (\autoref{sec:crfh}) or
{gist}~(\autoref{sec:gist}) try to describe the whole image. Either by
statistical analysis of features over all the image or by a structured
distribution of textures findable in the image.

\subsection{Local Features}
\label{sec:local-features}

\subsubsection{SIFT}
\label{sec:sift}
The detection of interesting points has been studied for several years and is
the base of several computer vision problems solution. It allows to perform
point matching which can be used in several areas from image stitching,
3D reconstruction, video tracking, object detection, etc\dots

The most used method was presented by \cite{lowe1999object}. And its based on
building a feature vector for each image. Each of those features is based on
\emph{interesting points} detected by detecting maxima and minima of a
difference of Gaussian functions applied in a scale-space.
The scale space is used to provide scale invariant detection. Gaussian functions
are used as they are the only way to model a linear scale-space.
Each interesting point is then described by a container that is rotation
invariant.

By seeing an object as a set of features points, index and matching is then
performed by a high-dimensional search on a database of know objects. After
matching objects can be verified for geometric coherence between features.
\Gls{SVM} classifiers can also be trained to detect objects based on this type
of local features by using \emph{Matching kernels} (\autoref{sec:kernel-trick}).

\begin{figure}[h]
    \includegraphics[width=\textwidth]{figures/sift/sift.pdf}
    \caption{{SIFT} and other local features have been proven useful in object
             detection.}
\end{figure}


\subsection{Global Features}

\subsubsection{Gist of a Scene}
\label{sec:gist}
\cite{oliva2006building} argue that fast scene recognition does not need to be
built on top of object recognition but can be analyzed by scene-centered
mechanisms.
They defend that position by pointing out behaviours on human vision:
when provided with a glance of a shot a person can identify the meaning of that
given shot or "gist of a scene" without remembering specific details.

\begin{figure}[h]
\center
\includegraphics[width=0.60\textwidth]{figures/gist.jpg}
\caption{An illustration of the gist of an image. Top row: original image I;
         bottom row: noise image J for which gist(I) = gist(J). We see that the
         gist captures the dominant textural features of the overall image, and
         their coarse spatial layout~\citep{murphy2006object}.}
\end{figure}


\subsubsection{{CRFH} - Composed Receptive Field Histograms}
\label{sec:crfh}\label{sec:global-features}
Composed Receptive Field Histograms are a multidimensional statistical
representation of the occurrence of several image descriptors applied to an
image. They can be seen as an high-dimension histogram where each cell records
how many pixels of the image have the cell response for the applied descriptors.
Such high-dimensional histogram is expected to be able to better global
information contained in the image by capturing several properties of the image
as well the relations between them.


\begin{figure}[h]
\begin{center}
\includegraphics[width=1\textwidth]{figures/crfh_model.jpg}
\end{center}
\caption{A two dimensional histogram of the image built out from two image
         descriptors: $Lx$ and $Ly$. First-order Gaussian derivatives of image
         luminance in horizontal and vertical direction applied at a scale 4.}
\end{figure}

Several type of image descriptors can be applied.
For example Gaussian derivatives (such as $L_x$, $L_y$, $L_{xx}$, $L_{yy}$) are
partial derivatives of the image luminance after applying a Gaussian filter of a
given scale ($\sigma$) on the image. Gradient magnitude descriptors
(such as $|\nabla L|$) can also be used as a rotation invariant descriptor.


One of the problems of high-dimensional histograms is the memory and
computationally complexity needed to handle them.
\cite{linde2004object} suggested using a sparse form to represent those by
storing only the non-zero-cells in an array structure.

Multidimensional histograms have proven to be useful in the context of object
recognition~\citep{schiele1996object}. And have also been previously used in
the context of visual place classification~\cite{pronobis2010ijrr}.
\emph{Histogram intersection kernels} (\autoref{sec:kernel-trick}) can be used
to train classifiers based on this feature.

\subsection{Laser Features}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Important sections of this chapter begin here.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Probability Theory}
\subsection{Bayes Theorem}

\subsection{Categorical and Multinomial Distribution}

\section{Probabilistic Graphical Models}
\label{sec:graphical-models}
% THIS IS ALL JUNK TEXT COPIED
Graphical models usage can be tracked back to earlies 1920 but they only become
popular in mid-eighties when researchers started to use \emph{Bayesian Networks}
to model expert systems~\citep{borgelt2002graphical}.

They serve as a better tool to model \emph{random variables}
(nodes on the graph) and their probabilities as they model the conditional
dependence between variables (edges on the graph). Important to note that here
\emph{random variables} does not denotes a truly random variable but one that is
unknown by the system and is conditioned by other variables/evidences.

This type of graphs provide a generative model where the probability of any
given scenario can be determined. As opposed to non-generative models where a
given probability can only be calculated if certain data is given.
This means when a graphical model is learned, it can be used to generate new
samples from the learned distribution.

They have been successfully used in several machine-learning task such as:
information extraction, speech recognition, computer vision.
They are also useful due to their ability to deal with semantic (high-level)
features~\citep{boutell2006factor} and ability to represent properties of the
reality they try to model.

Two main types of graphical models have been used: Bayesian Networks which model
directed edges between variables and Markov Random Fields where variables are
connected by a potential but no special direction is given to edges.

An important property of these graphs is the \emph{Markov-blanket} of a node.
For a given variable $a$, a \emph{Markov-blanket} is a set of variables in the
surroundings of $a$ that when given the value of $a$ becomes independent of the
rest of the graph~\citep{pearl1988probabilistic}.
On non-directional graphs it is directly determined by the nodes connected to $a$.
This allows the usage of graph algorithms such as \emph{min-cut} to quickly
determine most likely scenarios. In the case of directed edges a node blanket is
also influenced by the direction of the edges and a more complex schemes need to
be used.

As \cite{lauritzen2002chain} points this two types of models can be represented
as a \emph{chain graph} where both directed and undirected edges can co-exist.
Though this generalization is hard to implement due to mis-understandings on the
concepts the graph-models use.

Another useful interpretation of graphical-models are \emph{factor graphs}.
Those are able to handle both \emph{Bayesian Networks} and
\emph{Markov Random Fields}. Under this interpretation a graph is seen as a
bipartite graph that connects variables with factors that influence
them~\citep{bishop2006pattern}.
This gives a very useful framework to develop belief propagation on them by
seeing a message-passing mechanism between nodes. Belief propagation is used to
calculate marginal-probabilities.

\begin{figure}[ht]
    \begin{minipage}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/graphical-models/BayesNet.pdf}
        \caption{Bayes Networks}
    \end{minipage}
    \hspace{0.5cm}
    \begin{minipage}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=0.8\textwidth]{figures/graphical-models/MarkovRandomField.pdf}
        \caption{Markov Random Field}
    \end{minipage}
    \hspace{0.5cm}
    \begin{minipage}[b]{0.3\linewidth}
        \centering
        \includegraphics[width=\textwidth]{figures/graphical-models/FactorGraph.pdf}
        \caption{Factor Graph}
    \end{minipage}
\end{figure}

\subsection{Factor Graphs}
A \emph{factor graph} is a bipartite graph connecting two sets of nodes $X_G$ and $F_G$
representing random variables and factors.
Each factor is described by function $\phi$ dependent only on the variables $x_\phi$
to which the factor is connected.
Thus, a factor graph can be seen as a description of probability density function obtained
by a product of all the factors. In order to represent the probability,
a normalization factor needs to be introduced, resulting in the following equation:

\begin{equation}
P_G(x) = \frac{1}{Z}\prod_{\phi \in F_G}{\phi(x_{\phi})},\qquad
Z = \sum_{X_G}\prod_{\phi \in F_G}{\phi(x_{\phi})}
\end{equation}


More often we are only interested in a marginal of the probability distribution.

\begin{equation}
M_G(x) = \frac{1}{Z}\sum_{X_G \backslash x}{\prod_{\phi \in F_G}{\phi(x_{\phi})}}
\end{equation}

\subsection{Inference Engines}
Exact Inference and Approximate Inference methods

\section{Principle of Maximum Entropy}
\label{sec:max-entropy}
The principle of maximum entropy states that given a set of distributions that
are coherent with the acquired knowledge, the one which maximizes entropy should
be picked.

In the case of non-available information this is the uniform distributions.
In cases where we only know the mean and standard deviation a normal
distribution shall be picked, etc\dots

% In case we expand in some finding structure in graphs maybe the following will
% be useful
\section{Label Propagation, Min-Cuts}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Semantic Mapping}\label{chap:semantic-mapping}

% Advantages of Multi-modal approaches
% 

\section{Dora Architecture Overview}
\subsection{System organization}
\subsubsection{Sensory Layer}
\subsubsection{Categorical Layer}
\subsubsection{Place Layer}
\subsubsection{Conceptual Layer}

\section{Features}
\section{Conceptual Knowledge}
\section{Conceptual Map}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Novelty Detection}\label{chap:novelty-intro}

This chapter presents a statis

Laying down base concepts that are then used in \autoref{chap:novelty} to
implement novelty detection on the semantic spatial representation.

It start by presenting a brief overview on novelty detection.

It introduces novelty as a 

Shows novelty has an associated ordering.
Shows novelty has a threshold function.


At last a practical example using semantic data and probabilistic graphical
models is presented: it shows how to use the introduced ratio to obtain a
novelty detection system and analyses the performance impact by obtaining more
data and by using an approximation on the unconditional probability.


\begin{itemize}
\item Introduction
\item History
\item Approaches to Novelty Detection
\end{itemize}

\section{Novelty Detection Review and Related Work}
% Introduce novelty detection
Novelty detection deals with detecting that a new input was generated by a class
other than the ones the system knows about~\cite{markou2003novelty}.
It is harder than classification as only positive samples of a class are available
rendering normal classification methods invalid.

Adding capabilities to an agent to detect novel samples allows to increase its
reliability. Novelty signal can then be interpreted by the system to proceed
with more caution as its knowledge does not correctly describe reality.

% Explain threshold approach to novelty detection
Due to the desire of robustness, a novelty detection system is often given the option
to classify non-novel samples as novel. This is implemented as a threshold that describes a trade
between error and rejection: the more the system tries to reduce rejection, the more
prone to produce errors it becomes.



\section{Novelty Detection as a Threshold}
When dealing with a statistic point of view, noisy data or unstable features,
a decision to classify a sample $n$ as novel has an associated error probability
$P(\overline{novel}|n)$.
Accordingly, on a system that classifies a set of inputs $N$ as novel, the
probability of incorrectly classifying a sample as novel is given by the
equation:

\begin{equation}
\label{eq:false-positive}
P(\textnormal{false positive}|\textnormal{positive}) = \frac{\sum_{n \in N}{P(\overline{novel}|n)P(n)}}{\sum_{n \in N}{P(n)}}
\end{equation}

Therefore any novelty system that decides to acknowledge a sample
$a$ as novel should also detect novelty on any sample $b$ satisfying
$P(\overline{novel}|b) < P(\overline{novel}|a)$.
As that always reduce the error probability described in
\autoref{eq:false-positive} as shown:

\begin{equation}
\textnormal{Proof: to be optimal any system accepting }a\textnormal{ should also accept }b
\end{equation}

For the sake of completeness its show here that it also reduces the probability
of false negatives.

\begin{equation}
P(\textnormal{false negative}|\textnormal{negative}) = \frac{\sum_{n \notin N}{P(novel|n)P(n)}}{\sum_{n \notin N}{P(n)}}
\end{equation}


Based on this, it can be said that an optimal novelty detection system is
interested in defining an order relation on all the possible inputs equivalent
to the order defined by the error rate: $P(\overline{novel}|x)$.
And any optimal detector can be described by the largest $P(\overline{novel}|x)$
accepted by it. Which is seen as threshold.


\section{Conditional and Unconditional Probability Ratio}

On the previous section it was shown that an optimal novelty detector can be
implemented with a threshold on top of the order-relation defined by
$P(\overline{novel}|x)$ over $x$. Performing some manipulations with
Bayes theorem and assuming a constant $P(\overline{novel})$ a more usable
form can be attained:

\begin{equation}
\label{eq:novelty-ratio}
          P(\overline{novel}|x)
  =       \frac{P(x|\overline{novel}) P(\overline{novel})}{P(x)}
  \propto \frac{P(x|\overline{novel})}{P(x)}
\end{equation}

Since there is only interest in maintaining the same order-relation as
$P(\overline{novel}|x)$ the absolute value is not needed and any constant
factor can be dropped. Leaving a ratio between a \emph{conditional} and
\emph{unconditional probabilities} suitable for implementing novelty detection
via  thresholding.



\subsection{Conditional Probability}
The conditional probability $P(x|\overline{novel})$ describes the distribution
of the non-novel samples. In case the labelled data available for the agent
to learn a concept comes from this same distribution the correct approach
is to use it as prior-information for modelling the conditional probability.

Note that it is important for the labelled data to come from the same
distribution as where the system will run, as otherwise it will contain some
bias (for example introduced by an incorrect filtering)

Therefore the correct approach is to try to model it 


\subsection{Unconditional Probability}
The unconditional probability $P(x)$ plays an important role on obtaining a
correct order relation for performing novelty detection.
It serves as a normalizing component that allows the system to  whether
a given sample conditional probability arises from it belonging to the
known concept or from it likelihood of being sampled.


On lack of any information about the unconditional probability and conforming to
the principle of maximum entropy (\autoref{sec:max-entropy}) a uniform
distribution must be chosen.
Though, as discussed in \autoref{sec:unlabelled-data}, by using unlabelled data
it becomes possible to obtain prior-information and achieve a better
approximation.


%\subsection{Novelty Detection on a Variable Set of Features}
Note that very often, novelty detection is applied on a fixed set of features
together with an assumption of a uniform unconditional probability.
On those cases $P(x)$ becomes a constant and therefore a novelty threshold
can be directly applied on $P(x|\overline{novel})$ as is the case in \cite{bishop1994novelty}.
But in the case where the set of features $x$ is variable it cannot be
discarded. There $P(x)$ plays a role in levering all the conditional
probabilities on different sets of variables into the same measure units
such that a threshold can be implemented.



\section{A Practical Example}
\label{sec:unlabelled-data}
In order to verify the performance of the proposed threshold functions a synthetic dataset
was generated. To keep it simple only information regarding direct features of
a room were modelled and no structured knowledge such as room connectivity was taken
in account.
The synthetic distribution assumes that an independent and variable size set of features
$x$ is generated by a given room category.
In whole there was 11 different room categories and 7 different measured feature
types. Each feature type can be present more than once in (e.g.: room shape is
extracted from 2D laser scans in more than one position in the room).

The room categories were chosen to mimic as close as possible the real features and
categories existing in reality. And they model different room categories with
different levels of detail. For example: 1 person office, 2 person office, hallway,
robot lab.

From the distribution, 100 labelled samples for 5 of the 11 room categories were
drawn to represent the known concepts and 1000 unlabelled samples were drawn from
all the room categories for learning the unconditional probability distribution.
Using those samples, factors were learnt for the graphs used to model the
conditional distribution and the independent unconditional distribution.
\autoref{fig:simple-experiment} shows the graph structure used for approximate the
trained conditional and unconditional distributions.
Its important to notice that graph $G$ used to model the known classes when given
enough labelled data is able to exactly learn the conditional distribution as it
uses the same structure as the created synthetic distribution.

\begin{figure}[h]
\centering

\subfloat[Graph structure $G$.]{
         \includegraphics[width=0.27\textwidth]{figures/simple-cond-graph.pdf}}
\qquad
\subfloat[Graph structure $U$.]{
         \includegraphics[width=0.27\textwidth]{figures/simple-uniform-graph.pdf}}
\qquad
\subfloat[Graph structure $I$.]{
         \includegraphics[width=0.27\textwidth]{figures/simple-independent-graph.pdf}}

\caption{\label{fig:simple-experiment}The graph structures used to model the
         conditional and unconditional probability for implementing the novelty
         thresholds $P_G(x)/P_U(x)$ and $P_G(x)/P_I(x)$.}
\end{figure}

% Describe how to obtain the 3 thresholds functions seen on the graphs.
Using the learned models $G$, $U$ and $I$, two thresholds were trained:
$P_G(x)/P_U(x)$ assuming a uniform unconditional distribution
and $P_G(x)/P_I(x)$ assuming an independent unconditional distribution.
Since the distribution is synthetic there is access to $P(x)$ and $P(x|concept)$
and a perfect threshold function could also be created to test how far the
presented thresholds are from optimal.

\subsection{Probability Ratio Comparison}
%%% Results 1
% Show the threshold ratio is an optimal detector (if perfect information was available)
% Show that the thresholds are suitable functions for implementing a static threshold.
As first tryout, the performance of the novelty threshold selection was plotted for a set
of 1000 samples taken out from the whole distribution (\autoref{fig:synthetic-roc}).
Those samples contain different number of sensed properties for each room, mimicking
the dynamic properties expected to see when implemented on a robot.

\begin{figure}[h]
\centering
\includegraphics[width=0.60\textwidth]{results/synthetic-all.pdf}

\caption{\label{fig:synthetic-roc}ROC curve comparing novelty detection performance
         under samples with variable size of sensed properties.}
\end{figure}

The convex shape for the optimal threshold shows that the ratio between conditional
and unconditional probability is indeed an optimal detector and is suitable for
implementing a threshold when the samples are taken from dynamic
distributions (e.g.: some samples where there is only access to room size versus
samples where there is a lot of information about the room properties).

% Discuss importance on approximating unconditional probability.
Its also possible to see how important it is to estimate a correct unconditional
probability in order to obtain a correct novelty measure on the inputs.
The assumption of a uniform unconditional probability has led to very poor results.
That is probably explained by the semantic properties being highly
biased towards some values. And shows that bias plays an important step
in detecting whether a given sensed value is a valuable cue about the room category.



%%% Results 2
% Measure performance of the thresholds as more information becomes available.
\subsection{Performance Changes With Amount of Available Information}
In order to measure the performance impact as more semantic information becomes
available ROC curves were plotted for samples grouped by the number of sensed
semantic features.

\begin{figure}[h]
\centering

\subfloat[3 sensed features]{\includegraphics[width=0.40\textwidth]{results/synthetic-3features.pdf}}
\qquad
\subfloat[5 sensed features]{\includegraphics[width=0.40\textwidth]{results/synthetic-5features.pdf}}

\subfloat[10 sensed features]{\includegraphics[width=0.40\textwidth]{results/synthetic-10features.pdf}}
\qquad
\subfloat[50 sensed features]{\includegraphics[width=0.40\textwidth]{results/synthetic-50features.pdf}}

\caption{\label{fig:synthetic-roc-breakdown}ROC curves plotted showing performance of the
         presented novelty detection method on graphs generated for different amount of
         sensed features.}
\end{figure}

We can see that as the system gains more semantic information it becomes easier
to detect novelty. The input space size increases and allows the several existing
classes to become more easily distinguished.

The performance of the independent threshold decreases as the number of sensed
features increases. This is easily explained by the fact that the graph $I$ is not
able to model the existent dependence between the features. This becomes obvious
as the number of features increases (e.g.: graph $I$ perfectly models $P(x)$ in the
case where only 1 feature is sensed).

The uniform threshold shows a poor performance specially on small size samples
where its performs almost no better than random.
It performance increases as the size of sensed features increases but nonetheless
its very small when compared to how optimal a threshold could be.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Novelty Detection on Semantic Representations}\label{chap:novelty}
On the previous chapter, novelty as been introduced as a statistical
measure on the error-likelihood ratio between a conditional and unconditional
probability of a given .


\section{Detecting New Classes}
Explain exactly and formally how to obtain a novelty measure on a single
variable of a graph.\footnote{I am sure on the math of this.}

\section{Copping with Multiple Novel Variables}
On the previous section we show how to detect a novel class but that assumes
the rest of the graph holds true.

In this section we show how to handle novelty in several variables
in simultaneous.\footnote{I am almost sure the math here will work out the
way I think}

\subsection{Handling novelty signals from the low-level classifiers}
Explain how to handle novelty signals from low-level classifiers, now that we
have a model with the ability to still reason when several variables can be
novel.

\section{Detecting New Structure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusions and Future Work}\label{chap:conclusions}
\section{Future Work}

