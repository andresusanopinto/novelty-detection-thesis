\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[round]{natbib}
\include{macros}

\title{Notes on Semi-Supervised `Novelty Detection'}
\author{AndrÃ© Susano Pinto}

\begin{document}
\maketitle

\section{Introduction}
The case presented here assumes that a machine learning system besides having access to samples of the known class also has access to unlabelled data representative of the whole world.
This may happen in some machine learning problems where a set of samples from a given class has been grouped and identified and although its possible to gather new samples its infeasible to classify them (eg.: the human classifier got bored)\footnote{Other examples of situations are given on \autoref{sec:detection}.}.

Under that assumption its possible to model the functions $P(x|know)$ and $P(x)$ representing both a conditional and an unconditional density function for a given input.
Using those its possible to define a threshold function for novelty detection as seen on \autoref{sec:threshold}.

In \autoref{sec:threshold-analysis} the described threshold is analyzed in more detail, and in \autoref{sec:experiment} experiments to test the described methods are presented.

\subsection{(Novelty?) Detection}
\note{I believe calling \emph{novelty detection} to the threshold described here is incorrect.
I am not sure which would be the correct term.}

This new assumption of access to unlabelled data makes it dubious whether it can still be classified as a \emph{novelty detection} problem:
\begin{quotation}
`Novelty detection is the identification of new or unknown data or signal that a machine learning system is not aware of during training.' - \cite{markou2003novelty}.
\end{quotation}

But it is also different from a binary classification problem since instead of negative samples there is access to unlabelled data\footnote{Though several detection systems use binary classification techniques where negative samples are obtained by randomly sampling.}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Concept Detection}
\label{sec:detection}

\note{
This lacks literature background, and writing it on an airplane does not helps.
Gotta find some papers on the subject
}

In several machine learning problems the target is to detect whether a concept is present.
(eg.: face detection, intrusion detection).

In cases where negative samples are \emph{abundant} (face detection, music/speech detection), systems often use a random sample of input space to represent negative samples and then perform detection using a normal 2-class classification.

On systems where negative samples are very rare (intrusion-detection, cancer-detection), system need to use other approaches based on novelty detection\dots

Some cases lie somewhere in between: negative samples are easy to obtain, but they are not that common that we can assume a random sample does not contains positive samples in it. Hence we cannot treat a random sample as a negative.

In those cases we point to the usage of the ratio between conditional and unconditional probabilities as a detection function.
Examples of cases where this type of detection is may or may not appear are given below:

\subsubsection*{Unconstrainted labelled data}
On a system where a user is asked to tag samples with words representing concepts it is fairly easy to filter positive samples for a given concept.
Although getting negative samples is an impossible task, as the non-presence of a given tag on a sample does not necessarily makes that a negative sample for the concept.

Eg.: associating concepts with images, tagging objects on images, ...

\subsubsection*{Tag-prediction systems}
Music, image, text, news, auto-tagging\dots

\subsubsection*{Good approximation of density functions}
A good approximation of probability functions for a given input is necessary for the performance of the presented method.
So its expected to have uses where the input space is small or it can be modelled with some precision (eg.: independent features).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Threshold}
\label{sec:threshold}

The threshold is based on the probability that a given concept is present on a sample
knowing both that the concept samples and the unlabelled samples come from the same distribution $P$.

In that case our optimal threshold would be:
$P(concept | x) > 1/2$, which means we assume the concept is present if we have more than chance probability of getting it right.

Using bayes rules we obtain:

\begin{equation}\frac{P(x|concept) P(concept)}{P(x)} > \frac{1}{2}\end{equation}
\begin{equation}\frac{P(x|concept)}{P(x)} > \frac{1}{2 P(concept)}\end{equation}
\begin{equation}\frac{P(x|concept)}{P(x)} > K_{threshold}\end{equation}

\note{Novelty detection techniques often assume a constant $P(x)$ and then threshold is performed only on the conditional probability density function.}

Proposed detection method:
\begin{itemize}
\item From the environment draw labelled and unlabelled samples.
\item From those learn both $P(x|class \in known)$ and $P(x)$.
\item Define a detection threshold based on $P(x|class \in known)/P(x)$.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Threshold Analysis}
\label{sec:threshold-analysis}

\subsection{Bounding $P(c)$}
\note{This was just a random though... if we can bound $P(c)$ in some way then we get
      some information on how sure we are about a decision. Though its only possible
      to bound it from above, and even that not sure if its computational feasible.}

\begin{equation}P(x) = \sum_{c}^{} P(x|c) P(c)\end{equation}
\begin{equation}0 \le P(x|c) P(c) \le P(x)\end{equation}
\begin{equation}0 \le P(c) \le \frac{P(x)}{P(x|c)}\end{equation}


\subsection{What does a ROC curve looks like for an optimal detector}
\note{The ROC curve shape shows if a detector is perfect given an input space.
      (Or at least I think so (though I haven't write any proof)).
      I guess somewhere there should be literature reference to such a simple concept}

Proof would be something like:

An element of the input space is responsible for a vector change of $(TP, FP)$ in the ROC
curve.
A threshold functions orders the input space.
And the best is to take the greedy approach of sorting the input by $TP/FP$.
That is equivalent to a convex ROC curve.

This leads to the important property:

whatever threshold function you use,
if you get a convex ROC curve, there is no way to perform better using that input space.

At the same time we can proof an ordering on $TP/FP$ to be the same as $TP/(FP+TP)$.
Which is coherent with the threshold of: $P(x|concept)/P(x)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Evaluation and Experiments}
\label{sec:experiment}

This section describes the two experiences that were run to evaluate the methods.
\begin{description}
\item[\autoref{sec:simple-experiment}] presents a toy experience that aims at
  showing the importance of using $P(x)$ to estimate a novelty threshold and
  it importance when dealing with features that have a tendency to be highly
  biased such as the semantic data used on our sample.

\item[\autoref{sec:dora-experiment}] runs comparisons of the presented threshold
  on a real-world scenario, where a robot has to identify room categories based
  on obtained semantic data, such as object presence and room size.
  The experiment is run both with a fictitious distribution and with a real world
  dataset.
\end{description}

Other possible experiences are:
\begin{itemize}
\item run this type of classification on datasets such as \emph{LabelMe} databases,
  either by using user-supplied tags as semantic data or by extracting them with some
  image analysis.
\end{itemize}


\subsection{Simple}\label{sec:simple-experiment}
For a better understanding of how the knowledge of the unconditional probability $P(x)$ can help to improve the detection threshold
a very small toy experiment was run:

Using a simple distribution, described in \autoref{tab:simple-distribution},
labelled ($known = \{kitchen, corridor\}$)
and unlabelled samples where drawn.
$P(x|class \in known)$ and $P(x)$ where then approximated using simple accumulation tables
(only 6 different inputs).
The input space was then sorted according to the presented thresholds obtaining
the results as seen on \autoref{tab:simple-data-sorted}.

\begin{table}[hb]
\begin{center}
\input{simple-data/explain}
\end{center}
\caption{\label{tab:simple-distribution}Distribution used on the simple experiment. Each column cell shows $P(feature|class)$}
\end{table}

\begin{table}
\centering
\subfloat[Input space sorted by threshold $P(x|class \in known)$]{\input{simple-data/density_threshold}}
\qquad
\subfloat[Input space sorted by threshold $P(x|class \in known)/P(x)$]{\input{simple-data/semi_threshold}}
\caption{\label{tab:simple-data-sorted}Input space sorted by threshold functions.}
\end{table}

The results obtained demonstrate that using the conditional probability function
as a novelty threshold leads to poor performance: it classified (corridor,square) as
the most novel case.
This happens because the likelihood of such a sample is very low.
Nonetheless when using the factor $P(x)$ the system is able to take in consideration
that the low probability of that sample on the labelled data arrives from the low
probability of finding such a sample and not necessarily from a novel case.



\subsection{Dora with world model}
\label{sec:dora-experiment}

This experience is similar to the simple experiment run in \autoref{sec:simple-experiment}.
But this time using a more realistic distribution (\autoref{tab:dora-world-distribution}) with
11 different room categories and with a feature space containing semantic data for:
room size, room shape, visual appearance and 6 different objects.
Also, the features are statistically independent from each other given the room category.

The system objective was to detect novel data from a given known set of 6 categories.
Two tests where run:
\begin{enumerate}
\item Using perfect knowledge of the probability functions both thresholds where compared for performance (\autoref{tab:dora-data-perfect}).
\item A simple model was used to learn the probability functions and the thresholds were compared (\autoref{tab:dora-data-simple}).
\end{enumerate}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{dora-data/roc-perfect.pdf}
\caption{\label{tab:dora-data-perfect}ROC curve for thresholds based on perfect information running on the dora distribution}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=\textwidth]{dora-data/roc-simple.pdf}
\caption{\label{tab:dora-data-simple}ROC curve for thresholds learned on samples from samples in the dora distribution}
\end{figure}

\begin{sidewaystable}[h]
\begin{center}
\scalebox{0.35}{
\input{dora-data/explain}
}
\end{center}
\caption{\label{tab:dora-world-distribution}World model used in Dora. Each column cell shows $P(feature|class)$}
\end{sidewaystable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage
\phantomsection
\addcontentsline{toc}{section}{References}

\bibliographystyle{plainnat}
\bibliography{refs}

\end{document}

