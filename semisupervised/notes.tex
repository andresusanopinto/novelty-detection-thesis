\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[round]{natbib}
\include{macros}

\title{Notes on Semi-Supervised `Novelty Detection'}
\author{AndrÃ© Susano Pinto}

\begin{document}
\maketitle

\section{Introduction}
The case presented here assumes that a machine learning system besides having access to samples of the known class also has access to unlabelled data representative of the whole world.
This may happen in some machine learning problems where a set of samples from a given class has been grouped and identified and although its possible to gather new samples its infeasible to classify them (eg.: the human classifier got bored)\footnote{Other examples of situations are given on \autoref{sec:detection}.}.

Under that assumption its possible to model the functions $P(x|know)$ and $P(x)$ representing both a conditional and an unconditional density function for a given input.
Using those its possible to define a threshold function for novelty detection as seen on \autoref{sec:threshold}.

In \autoref{sec:threshold-analysis} the described threshold is analyzed in more detail, and in \autoref{sec:experiment} experiments to test the described methods are presented.

\subsection{(Novelty?) Detection}
\note{I believe calling \emph{novelty detection} to the threshold described here is incorrect.
I am not sure which would be the correct term.}

This new assumption of access to unlabelled data makes it dubious whether it can still be classified as a \emph{novelty detection} problem:
\begin{quotation}
`Novelty detection is the identification of new or unknown data or signal that a machine learning system is not aware of during training.' - \cite{markou2003novelty}.
\end{quotation}

But it is also different from a binary classification problem since instead of negative samples there is access to unlabelled data\footnote{Though several detection systems use binary classification techniques where negative samples are obtained by randomly sampling.}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Concept Detection}
\label{sec:detection}

\note{
This lacks literature background, and writing it on an airplane does not helps.
Gotta find some papers on the subject
}

In several machine learning problems the target is to detect whether a concept is present.
(eg.: face detection, intrusion detection).

In cases where negative samples are \emph{abundant} (face detection, music/speech detection), systems often use a random sample of input space to represent negative samples and then perform detection using a normal 2-class classification.

On systems where negative samples are very rare (intrusion-detection, cancer-detection), system need to use other approaches based on novelty detection\dots

Some cases lie somewhere in between: negative samples are easy to obtain, but they are not that common that we can assume a random sample does not contains positive samples in it. Hence we cannot treat a random sample as a negative.

In those cases we point to the usage of the ratio between conditional and unconditional probabilities as a detection function.
Examples of cases where this type of detection is may or may not appear are given below:

\subsubsection*{Unconstrainted labelled data}
On a system where a user is asked to tag samples with words representing concepts it is fairly easy to filter positive samples for a given concept.
Although getting negative samples is an impossible task, as the non-presence of a given tag on a sample does not necessarily makes that a negative sample for the concept.

Eg.: associating concepts with images, tagging objects on images, ...

\subsubsection*{Tag-prediction systems}
Music, image, text, news, auto-tagging\dots

\subsubsection*{Good approximation of density functions}
A good approximation of probability functions for a given input is necessary for the performance of the presented method.
So its expected to have uses where the input space is small or it can be modelled with some precision (eg.: independent features).


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Threshold}
\label{sec:threshold}

The threshold is based on the probability that a given concept is present on a sample
knowing both that the concept samples and the unlabelled samples come from the same distribution $P$.

In that case our optimal threshold would be:
$P(concept | x) > 1/2$, which means we assume the concept is present if we have more than chance probability of getting it right.

Using bayes rules we obtain:

\begin{equation}\frac{P(x|concept) P(concept)}{P(x)} > \frac{1}{2}\end{equation}
\begin{equation}\frac{P(x|concept)}{P(x)} > \frac{1}{2 P(concept)}\end{equation}
\begin{equation}\frac{P(x|concept)}{P(x)} > K_{threshold}\end{equation}

\note{Novelty detection techniques often assume a constant $P(x)$ and then threshold is performed only on the conditional probability density function.}

Proposed detection method:
\begin{itemize}
\item From the environment draw labelled and unlabelled samples.
\item From those learn both $P(x|class \in known)$ and $P(x)$.
\item Define a detection threshold based on $P(x|class \in known)/P(x)$.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Threshold Analysis}
\label{sec:threshold-analysis}

\begin{equation}P(x) = \sum_{c}^{} P(x|c) P(c)\end{equation}

\begin{equation}0 \le P(x|c) P(c) \le P(x)\end{equation}
\begin{equation}0 \le P(c) \le \frac{P(x)}{P(x|c)}\end{equation}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
\label{sec:experiment}
\begin{itemize}
\item Define a set of features (all extractable from visual or user-tags).
\item Define a dataset of inputs (eg.: indoor environs at LabelMe)
\item Filter those for \emph{kitchen}.
\item Use ML methods to model $P(features | kitchen)$ and $P(features)$.
\item Test results.
\end{itemize}


\subsection{Simple}
\label{sec:simple-experiment}
\note{This first experience is very small and was only used to better explain the effects of the threshold functions.}

For a better understanding of how the knowledge of the unconditional probability $P(x)$ can help to improve the detection threshold
a very small toy experiment was run:

Using a simple distribution, described in \autoref{tab:simple-distribution},
labelled ($known = \{kitchen, corridor\}$)
and unlabelled samples where drawn.
$P(x|class \in known)$ and $P(x)$ where then approximated using simple accumulation tables
(only 6 diferent inputs).
The input space was then sorted according to the presented thresholds obtaining
the results as seen on \autoref{tab:simple-data-sorted}.

\begin{table}
\begin{center}
\input{simple-data/explain}
\end{center}
\caption{\label{tab:simple-distribution}Distribution used on the simple experiment. Each column cell shows $P(feature|class)$}
\end{table}

\begin{table}
\centering
\subfloat[Input space sorted by threshold $P(x|class \in known)$]{\input{simple-data/density_threshold}}
\qquad
\subfloat[Input space sorted by threshold $P(x|class \in known)/P(x)$]{\input{simple-data/semi_threshold}}
\caption{\label{tab:simple-data-sorted}Input space sorted by threshold functions.}
\end{table}

The results obtained demonstrate that using the conditional probability function
as a novelty threshold leads to poor performance: it classified (corridor,square) as
the most novel case.
This happens because the likelihood of such a sample is very low.
Nonetheless when using the factor $P(x)$ the system is able to take in consideration
that the low probability of that sample on the labelled data arrives from the low
probability of finding such a sample and not necessarily from a novel case.

This simple toy experience demonstrates the importance of using $P(x)$ to estimate a novelty
threshold and it importance when dealing with features that have a tedency to be highly bias
such as the semantic data used on our sample.



\subsection{Dora with world model}

This experience is similar to the simple experiment run in \autoref{sec:simple-experiment}.
But this time using a more realistic distribution (\autoref{tab:dora-world-distribution}) with
11 diferent room categories and with a feature space containing semantic data for:
room size, room shape, visual appearance and 6 diferent objects.
Also, the features are statistically independent from each other given the room category.

The system objective was to detect novel data from a given known set of 6 categories.
In order to use drawn results two tests where run:
\begin{enumerate}
\item Using perfect knowledge of the probability functions both thresholds where compared for performance (\autoref{tab:dora-data-perfect}).
\item A simple model was used to learn the probability functions and the thresholds were compared (\autoref{tab:dora-data-simple}).
\end{enumerate}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{dora-data/roc-perfect.pdf}
\caption{\label{tab:dora-data-perfect}ROC curve for thresholds based on perfect information running on the dora distribution}
\end{figure}

\begin{figure}
\centering
\include{dora-data/roc-simple.tex}
\caption{\label{tab:dora-data-simple}ROC curve for thresholds learned on samples from samples in the dora distribution}
\end{figure}

\begin{sidewaystable}[h]
\begin{center}
\scalebox{0.35}{
\input{dora-data/explain}
}
\end{center}
\caption{\label{tab:dora-world-distribution}World model used in Dora. Each column cell shows $P(feature|class)$}
\end{sidewaystable}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\cleardoublepage
\phantomsection
\addcontentsline{toc}{section}{References}

\bibliographystyle{plainnat}
\bibliography{refs}

\end{document}

