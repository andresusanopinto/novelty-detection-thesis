\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}

\usepackage{url}
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

% Additional packages
\usepackage[utf8]{inputenc}
\usepackage[round]{natbib}
\usepackage{subfig}
\usepackage{tikz}
\usepackage[pdftex,pdfpagelabels,bookmarks,hyperindex,hyperfigures]{hyperref}
\hypersetup{%
   plainpages=false, 
   pdfpagelayout=SinglePage,
   bookmarksopen=false,
   bookmarksnumbered=true,
   breaklinks=true,
   linktocpage,
   colorlinks=true,
   linkcolor=blue,
   urlcolor=blue,
   citecolor=blue,
   anchorcolor=green
}      

% TiKZ styles
\tikzstyle{var} = [circle,
                   thick,
                   draw, fill=red!20,
                   font=\scriptsize,
                   text width=3.5em,
                   text centered,
                   node distance=1em,
                   inner sep=0pt]

\tikzstyle{factor} = [rectangle,
                      thick,
                      draw, fill=blue!20,
                      minimum height=2em,
                      minimum width=2em]

\tikzstyle{line} = [draw]

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Semantic Novelty Detection with Graphical Models}

% a short form should be given in case it is too long for the running head
% \titlerunning{Lecture Notes in Computer Science: Authors' Instructions}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Andr√© Susano Pinto}
%
%\authorrunning{Lecture Notes in Computer Science: Authors' Instructions}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Faculdade de Engenharia da Universidade do Porto, Portugal\\
\url{andresusanopinto@gmail.com}\\
\url{http://url}}

\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle


\begin{abstract}
This papers presents an approach on implementing novelty detection at semantic level
for indoor room classification in Dora. \emph{Graphical models} are used to model
probabilistic knowledge and novelty detection is implemented on top of them.
The novelty threshold is then optimized using an unconditional probability density
model that is trained from unlabelled data.

\keywords{novelty detection, semantic data, graphical models, room classification,
indoor environments, robotics, multi-modal.}
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\begin{itemize}
\item Explain context of indoor room categorization and importance of novelty detection.
\item Introduce Dora and explain how it would benefit or use the semantic novel detection.
\end{itemize}

There has been several efforts in the area of Artificial Intelligence and Robotics in creating robots
that are able to interact with humans and their environments.
One of the existing problems is a reliable high-level localization method that can be deployed into new
and unknown environments.
In this article we focus on the mapping, using \emph{semantic data}, of indoor environments such as houses and offices to room categories
such as kitchen, corridor, office.
And how to perform novelty detection on them: \emph{detect a new room category that was not present in the labelled data}.

Dora\cite{dora} (CogX: Dora) was used as a base system where this novelty detection would be implemented.
As Dora moves through the environment its \emph{conceptual layer} builds a structural and probabilistic representation of the space
instantiated as a \emph{graphical model}.
That model connects the sensed properties together with the variables used to model the world.
And allows to perform queries on the probabilities of aspects of the environment.
For example: where is most likely to find a cereal box\cite{exploiting}.

The ability to recognize novel categories on the modelled variables would allow to increase
reliability and allow the creation of self-extending behaviours.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\cite{quattoni2009recognizing} showed that most scene recognition models work poorly in indoor
scenes when compared to outdoor scenes results.
Since the properties that characterize rooms changes conforming its category. Namely corridors are
well described by global properties and bookstores are well described by the presence of specific objects (books).
Their work shows a multi-modal approach is expected to yield better performance by mixing several sources of information.

\cite{galindo2005multi} defines a bidirectional relation between object and room category, where object defines a room category and a room category provides information on where objects may be found.

Probabilistic representations are used in several localised functions in robots operating in the real-world~\cite{gross2009toomas,maierprobabilistic}. And some employ, up to some extent, a probabilistic representation across some subsystems~\cite{kraft2008exploration}.
\cite{vasudevan2008bayesian} performed room categorization through Bayesian reasoning about the presence of objects but did not included observations models (perception is considered deterministic).
And \cite{boutell2006factor} have studied outdoor scene classification using \emph{factor graphs} and modelling spatial relations between objects in the scene to extract better knowledge from semantic (high-level) features.

Its expected that using a unified probabilistic model from the whole system, such as \cite{pronobis2011exploiting}, more information can be reused to correctly predict a given random variable.

Although this paper deals with novelty detection on the robotics area, it does so using very
standard concepts and techniques such as semantic data and graphical models.
Those are often found on areas related with information retrieval.
Interesting examples are works in automatic image annotation using an hidden concept layer between visual features and text information\cite{zhang2005probabilistic}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dora Architecture Overview}
\begin{itemize}
\item Introduce Dora architecture aspects important for the paper
\item Explain what we mean by semantic data and how its captured by Dora
\item Introduce graphical models and how we use them (including notation on the paper)
\end{itemize}

Short paragraph describing dora. Short paragraph describing dora. Short paragraph describing dora. Short paragraph describing dora. Short paragraph describing dora.

From its architecture only the \emph{conceptual layer} is of interest to this article.
Its role is to aggregate the following semantic information coming from other layers:

\begin{description}
 \item[Doorway detection] is used to segment the low-level space into rooms and map connectivity between them.
 \item[Room size and shape] are classified by using 2D laser scans data and are associated as properties of a given room. The system utilizes pre-trained set of classifiers to label size as (large, medium, small) and shape as (rectangular or elongated).
 \item[Object detection] is performed using the visual input in order to detect . The system keeps track of the number of object and types seen per each room. Objects are once again detected by running a pre-trained set of detectors for: book, cereal box, computer, robot, stapler, toilet paper.
 \item[Room appearance] is categorized from the visual input by using CRFH and a pre-trained set of 7 different models.
\end{description}

With the extracted information and conceptual knowledge the conceptual layer creates a structured probabilistic representation
in order to model all known variables and their relations.
As so, in order to represent the knowledge the layer builds a \emph{chain graph}:
a probabilistic graphical model that merges both Bayesian Networks and Random Markov Fields.

The use of graphical models to describe distributions of variables has useful properties.
The edges between the variables can be seen as a kind of filter to the properties of the system.
Properties those that we expect to capture by the conceptual knowledge.
At the same time they are a generative models and therefore allow to calculate the probability
on any given subset of variables on the graph allowing the system to work even when some information is missing.


\begin{figure}[h]
\centering

\includegraphics[width=0.50\textwidth]{figures/conceptual-layer.jpg}
\includegraphics[width=0.49\textwidth]{figures/chain-graph.png}
\caption{The conceptual layer structures the sensed environment together with the conceptual knowledge in order to
         create a structured probabilistic representation of the world.}
\end{figure}

\subsection{Factor Graphs}
Although the conceptual layer works with \emph{chain graph}, those can be converted into \emph{factor graphs}.
We opted to use those as describing the ideas on this paper as they tend to be easier to understand.

A \emph{factor graph} is a bipartite graph connecting nodes representing random variables and factors.
Each factor represents a function dependent only on the variables to where its connected.
As so given factor graph $G$ can be seen as description of a distribution function over a set of variables obtained by
the product of all the factors. This manipulation allows to use algorithms such as the Sum-Product to efficiently calculate
probabilities on any given subset $x$ of those variables by exploiting conditional independency between variables.
We will use the notation $P_G(x)$ to represent that.

\begin{figure}[h]
\centering
\begin{tikzpicture}[node distance = 2cm, auto]
    \matrix[row sep=0.5cm,column sep=0.5cm] {
      & \node [var] (room) {Room Category}; \\
      \node [factor] (fAppearance) {}; &
      \node [factor] (fShape) {}; &
      \node [factor] (fSize) {}; \\

      \node [var] (appearance) {Appearance}; &
      \node [var] (shape) {Shape}; &
      \node [var] (size) {Size}; \\
    };
  \path [line] (room) -- (fAppearance);
  \path [line] (room) -- (fShape);
  \path [line] (room) -- (fSize);

  \path [line] (fAppearance) -- (appearance);
  \path [line] (fShape) -- (shape);
  \path [line] (fSize) -- (size);
\end{tikzpicture}
\caption{Example of a factor graph used for Dora for representing the environment.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Novelty Detection}
\begin{itemize}
\item Define novelty detection 
\item Explain approach with a threshold function
\item Need of a normalizing factor for a threshold with dynamic functions
\item Usage of conditional density
\item Assuming of every output is equally likely
\item Using unlabelled data
\item What happens if the conditional and unconditional graphs differ only by the addition of a node?
\item How to implement that in the case a graph is a tree
\item Novelty detection on any variable of the graphical model!
\end{itemize}

% Introduce novelty detection
Novelty detection is an harder problem than that of classification due to its single-class nature.
Essentially a system is given training data and when given new data has to decide if it belongs
to the same concept it was trained with.

% Explain threshold approach to novelty detection
Due to the desired of robustness and generalization, a novelty detection system would always possess
some threshold that describes on how strict the system should be to new data.
This threshold is often seen as a distance measure to a normal input and several novelty detection
techniques are based on it.

When dealing with a statistic point of view, noisy data or unstable features, a decision on a given input has
an associated error rate. And in this case a novelty detection system is interested in defining an order
on all the possible inputs equivalent to the order imposed by the error rate: $P(novel|x)$.

Such an ordering can be defined by a R-value function on the input space. And then a threshold can be trained
on that function in order to accept an expected amount of cases at the cost of failing in others.

Based on bayes rule:

\begin{equation}
P(\overline{novel}|x) = \frac{P(x|\overline{novel}) P(\overline{novel})}{P(x)}
\end{equation}

% Explain approach of using conditional probability
\subsection{Conditional Probability: $P(x|\overline{novel})$}
One of the hardest problems is that on most cases novelty detection system only have access to 
the known data.
Under that the best approach is to define a threshold based on $P(x|\overline{novel})/P(x)$
considering that $P(x)$ is constant through all the input space.

In the presented case, $P(x|\overline{novel})$ is approximated by the graphical model $G$ produced
by the conceptual layer. As so we use $P_G(x)$ as a substitute.

\label{sec:uniform-unconditional}
In several cases assuming the unconditional probability $P(x)$ is constant leads to discarding the factor,
nonetheless here the distributions are dynamically changing as the robot learns more on the environment.
So we need to keep the normalizing argument $P(x)$ and re-calculate it for each new subset of $x$.
Still assuming that the unconditional distributions generates all possible outcome with the same probability
we can model it $P(x)=\prod{|x_i|}$, where $|x_i|$ denotes the state space size of variable $x_i$.
In graphical model terms, this would be equivalent to build a factor graph $G'$ with the same variables but
without any factors.

Using a graphical model built to model the known data and assuming that in the unconditional case all possible outcomes
are equally likely a novelty threshold would be given by: $P_G(x)/P_{G'}(x)$, as illustrated on \autoref{fig:unsupervised-threshold}.

\begin{figure}[h]
\centering
\subfloat[Factor graph $G$ used to model conditional probability]{
\begin{tikzpicture}
  \matrix[row sep=1em,column sep=1.5em, ampersand replacement=\&] {
    \& \node [var] (room) {Room Category}; \& \\

    \node [factor] (fAppearance) {}; \&
    \node [factor] (fShape) {}; \&
    \node [factor] (fSize) {}; \\
    
    \node [var] (appearance) {Appearance}; \&
    \node [var] (shape) {Shape}; \&
    \node [var] (size) {Size}; \\
  };
  \path [line] (room) -- (fAppearance);
  \path [line] (room) -- (fShape);
  \path [line] (room) -- (fSize);

  \path [line] (fAppearance) -- (appearance);
  \path [line] (fShape) -- (shape);
  \path [line] (fSize) -- (size);
\end{tikzpicture}}
\hfill
\subfloat[Factor graph $G'$ used to model a unconditional probability distribution]{
\begin{tikzpicture}
  \matrix[row sep=1em,column sep=1.5em, ampersand replacement=\&] {
    \& \node [var] (room) {Room Category}; \& \\

    \node [var] (appearance) {Appearance}; \&
    \node [var] (shape) {Shape}; \&
    \node [var] (size) {Size}; \\
  };
\end{tikzpicture}}

\caption{\label{fig:unsupervised-threshold}
         Assuming a uniform unconditional distribution of any subset of the variables $x$,
         a novelty threshold can be defined on the function $P_G(x)/P_{G'}(x)$.}
\end{figure}


\subsection{Using Unlabelled Data}
Nonetheless in several cases its often the case that there is access to large amounts of unlabelled data.
Under that it becomes possible to obtain a better approach to the unconditional probability distribution that
the uniform one assumed in \autoref{sec:uniform-unconditional}.

For interesting and practical reasons it was assumed that the unconditional distribution could be modelled with
all the variables independent from each other.
Which is the same as assuming $P(x)=\prod{P_{x_i}(x_i)}$.
And so from the unlabelled data we only need to estimate $P_{x_i}$.
Once again \autoref{fig:semisupervised-threshold} illustrates this in terms of graphs $G$ and $G'$.

\begin{figure}[h]
\centering
\subfloat[Factor graph $G$ used to model conditional probability]{
\begin{tikzpicture}
  \matrix[row sep=1em,column sep=1.5em, ampersand replacement=\&] {
    \& \node [var] (room) {Room Category}; \& \\

    \node [factor] (fAppearance) {}; \&
    \node [factor] (fShape) {}; \&
    \node [factor] (fSize) {}; \\
    
    \node [var] (appearance) {Appearance}; \&
    \node [var] (shape) {Shape}; \&
    \node [var] (size) {Size}; \\
  };
  \path [line] (room) -- (fAppearance);
  \path [line] (room) -- (fShape);
  \path [line] (room) -- (fSize);

  \path [line] (fAppearance) -- (appearance);
  \path [line] (fShape) -- (shape);
  \path [line] (fSize) -- (size);
\end{tikzpicture}}
\hfill
\subfloat[Factor graph $G'$ used to model a unconditional probability distribution]{
\begin{tikzpicture}
  \matrix[row sep=1em,column sep=1.5em, ampersand replacement=\&] {
    \node [var] (appearance) {Appearance}; \&
    \node [var] (shape) {Shape}; \&
    \node [var] (size) {Size}; \\

    \node [factor] (fAppearance) {}; \&
    \node [factor] (fShape) {}; \&
    \node [factor] (fSize) {}; \\
  };
  \path [line] (fAppearance) -- (appearance);
  \path [line] (fShape) -- (shape);
  \path [line] (fSize) -- (size);
\end{tikzpicture}}

\caption{\label{fig:semisupervised-threshold}Using unlabelled data, its possible to create a better approximation
         for the unconditional probability. Which should lead to a better novelty threshold.}
\end{figure}

The described ratio between the two graphs can be understood as compensating for an existing bias
on the unconditional distribution.
Which is an important step to achieve a correct ordering of the input space for novelty thresholding.
This way our unconditional function perceive which features are slightly more biased to a certain value and will
compensate on the probability calculation.

\subsection{Generalizing to Novelty Detection on a variable}
Until now, the presented methods describe how to detect data that belongs to a new class of input by creating
generative models for both conditional and unconditional probabilities distributions.

And although it serves as a method for detecting new classes in the input, it gives us no control in what
variables novelty is being measured.
This is the novelty can come from any variable present on the graphs and we have no way to measure it.

Lets assume that the only difference between the conditional and unconditional graph is a single node and its connecting factors
as seen on \autoref{fig:measure-room-novelty}.

\begin{figure}
\caption{\label{fig:measure-room-novelty}In this case the only difference between $G$ and $G'$ is the room category variable,
         that only exists in the later.}
\end{figure}


From now on we refer to the measure $P_G(x)/P_{G'}(x)$, when $G$ and $G'$ as a novelty measure on the variable $R_{category}$.
As it attempts to measure on how much adding the variable $R_{category}$ helps to explain the given distribution of values.

Expanding the calculation of $P_G(x)/P_{G'}(x)$ with the sum-product technique. Leads to:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\begin{itemize}
\item Show results and how much it improves by using a simple method for unconditional probability
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Future Work}
\begin{itemize}
\item Show a correct estimation of unconditional probability to be important in estabilishing a novelty detection threshold
\item Show unlabelled data can be used for it.
\item Although focus on Room Categorization we expect the results and techniques to be usable on other areas.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plainnat}
\bibliography{refs}

\end{document}
